output = ('Hugging Face\nModels Datasets Spaces Docs\nSolutions\nPricing\nLog In Sign Up\nTransformers documentation Trainer\nTransformers\nSearch documentation Ctrl+K mainv4.35.0v4.34.1v4.33.3v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-builder-html DEENESFRHIITJAKOPTTEZH\nGet started\n🤗 Transformers Quick tour Installation\nTutorials\nRun inference with pipelines Write portable code with AutoClass Preprocess data Fine-tune a pretrained model Train with a script Set up distributed training with 🤗 Accelerate Load and train adapters with 🤗 PEFT Share your model Agents Generation with LLMs\nTask Guides\nNatural Language Processing\nAudio\nComputer Vision\nMultimodal\nGeneration\nPrompting\nDeveloper guides\nUse fast tokenizers from 🤗 Tokenizers Run inference with multilingual models Use model-specific APIs Share a custom model Templates for chat models Run training on Amazon SageMaker Export to ONNX Export to TFLite Export to TorchScript Benchmarks Notebooks with examples Community resources Custom Tools and Prompts Troubleshoot\nPerformance and scalability\nOverview Efficient training techniques\nMethods and tools for efficient training on a single GPU Multiple GPUs and parallelism Efficient training on CPU Distributed CPU training Training on TPUs Training on TPU with TensorFlow Training on Specialized Hardware Custom hardware for training Hyperparameter Search using Trainer API\nOptimizing inference\nCPU inference GPU inference\nInstantiating a big model Troubleshooting XLA Integration for TensorFlow Models Optimize inference using `torch.compile()`\nContribute\nHow to contribute to transformers? How to add a model to 🤗 Transformers? How to convert a 🤗 Transformers model to TensorFlow? How to add a pipeline to 🤗 Transformers? Testing Checks on a Pull Request\nConceptual guides\nPhilosophy Glossary What 🤗 Transformers can do How 🤗 Transformers solve tasks The Transformer model family Summary of the tokenizers Attention mechanisms Padding and truncation BERTology Perplexity of fixed-length models Pipelines for webserver inference Model training anatomy Getting the most out of LLMs\nAPI\nMain Classes\nAgents and Tools Auto Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Text Generation ONNX Optimization Model outputs Pipelines Processors Quantization Tokenizer Trainer DeepSpeed Integration Feature Extractor Image Processor\nModels\nText models\nVision models\nAudio models\nMultimodal models\nReinforcement learning models\nTime series models\nGraph models\nInternal Helpers\nCustom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation Utilities for Image Processors Utilities for Audio processing General Utilities Utilities for Time Series\nJoin the Hugging Face community\nand get access to the augmented documentation experience\nCollaborate on models, datasets and Spaces\nFaster examples with accelerated inference\nSwitch between documentation themes\nSign Up\nto get started\nTrainer The Trainer class provides an API for feature-complete training in PyTorch for most standard use cases. It’s used in most of the example scripts. If you’re looking to fine-tune a language model like Llama-2 or Mistral on a text dataset using autoregressive techniques, consider using trl’s SFTTrainer. The SFTTrainer wraps the Trainer and is specially optimized for this particular task and supports sequence packing, LoRA, quantization, and DeepSpeed for efficient scaling to any model size. On the other hand, the Trainer is a more versatile option, suitable for a broader spectrum of tasks. Before instantiating your Trainer, create a TrainingArguments to access all the points of customization during training. The API supports distributed training on multiple GPUs/TPUs, mixed precision through NVIDIA Apex and Native AMP for PyTorch. The Trainer contains the basic training loop which supports the above features. To inject custom behavior you can subclass them and override the following methods: get_train_dataloader — Creates the training DataLoader. get_eval_dataloader — Creates the evaluation DataLoader. get_test_dataloader — Creates the test DataLoader. log — Logs information on the various objects watching training. create_optimizer_and_scheduler — Sets up the optimizer and learning rate scheduler if they were not passed at\ninit. Note, that you can also subclass or override the create_optimizer and create_scheduler methods\nseparately. create_optimizer — Sets up the optimizer if it wasn’t passed at init. create_scheduler — Sets up the learning rate scheduler if it wasn’t passed at init. compute_loss - Computes the loss on a batch of training inputs. training_step — Performs a training step. prediction_step — Performs an evaluation/test step. evaluate — Runs an evaluation loop and returns metrics. predict — Returns predictions (with metrics if labels are available) on a test set. The Trainer class is optimized for 🤗 Transformers models and can have surprising behaviors\nwhen you use it on other models. When using it on your own model, make sure: your model always return tuples or subclasses of ModelOutput. your model can compute the loss if a labels argument is provided and that loss is returned as the first\nelement of the tuple (if your model returns tuples) your model can accept multiple label arguments (use the label_names in your TrainingArguments to indicate their name to the Trainer) but none of them should be named "label". Here is an example of how to customize Trainer to use a weighted loss (useful when you have an unbalanced training set):\nCopied from torch import nn\nfrom transformers import Trainer\nclass CustomTrainer(Trainer):\ndef compute_loss(self, model, inputs, return_outputs=False):\nlabels = inputs.pop("labels")\n# forward pass\noutputs = model(**inputs)\nlogits = outputs.get("logits")\n# compute custom loss (suppose one has 3 labels with different weights)\nloss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))\nloss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\nreturn (loss, outputs) if return_outputs else loss Another way to customize the training loop behavior for the PyTorch Trainer is to use callbacks that can inspect the training loop state (for progress reporting, logging on TensorBoard or other ML platforms…) and take decisions (like early stopping).\nTrainer class transformers.Trainer\n< source > ( model: typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = Noneargs: TrainingArguments = Nonedata_collator: typing.Optional[DataCollator] = Nonetrain_dataset: typing.Optional[torch.utils.data.dataset.Dataset] = Noneeval_dataset: typing.Union[torch.utils.data.dataset.Dataset, typing.Dict[str, torch.utils.data.dataset.Dataset], NoneType] = Nonetokenizer: typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = Nonemodel_init: typing.Union[typing.Callable[[], transformers.modeling_utils.PreTrainedModel], NoneType] = Nonecompute_metrics: typing.Union[typing.Callable[[transformers.trainer_utils.EvalPrediction], typing.Dict], NoneType] = Nonecallbacks: typing.Optional[typing.List[transformers.trainer_callback.TrainerCallback]] = Noneoptimizers: typing.Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None)preprocess_logits_for_metrics: typing.Union[typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], NoneType] = None )\nExpand 11 parameters Parameters\nmodel (PreTrainedModel or torch.nn.Module, optional) —\nThe model to train, evaluate or use for predictions. If not provided, a model_init must be passed.\nTrainer is optimized to work with the PreTrainedModel provided by the library. You can still use\nyour own models defined as torch.nn.Module as long as they work the same way as the 🤗 Transformers\nmodels.\nargs (TrainingArguments, optional) —\nThe arguments to tweak for training. Will default to a basic instance of TrainingArguments with the\noutput_dir set to a directory named tmp_trainer in the current directory if not provided.\ndata_collator (DataCollator, optional) —\nThe function to use to form a batch from a list of elements of train_dataset or eval_dataset. Will\ndefault to default_data_collator() if no tokenizer is provided, an instance of\nDataCollatorWithPadding otherwise.\ntrain_dataset (torch.utils.data.Dataset or torch.utils.data.IterableDataset, optional) —\nThe dataset to use for training. If it is a Dataset, columns not accepted by the\nmodel.forward() method are automatically removed.\nNote that if it’s a torch.utils.data.IterableDataset with some randomization and you are training in a\ndistributed fashion, your iterable dataset should either use a internal attribute generator that is a\ntorch.Generator for the randomization that must be identical on all processes (and the Trainer will\nmanually set the seed of this generator at each epoch) or have a set_epoch() method that internally\nsets the seed of the RNGs used.\neval_dataset (Union[torch.utils.data.Dataset, Dict[str, torch.utils.data.Dataset]), optional) —\nThe dataset to use for evaluation. If it is a Dataset, columns not accepted by the\nmodel.forward() method are automatically removed. If it is a dictionary, it will evaluate on each\ndataset prepending the dictionary key to the metric name.\ntokenizer (PreTrainedTokenizerBase, optional) —\nThe tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the\nmaximum length when batching inputs, and it will be saved along the model to make it easier to rerun an\ninterrupted training or reuse the fine-tuned model.\nmodel_init (Callable[[], PreTrainedModel], optional) —\nA function that instantiates the model to be used. If provided, each call to train() will start\nfrom a new instance of the model as given by this function.\nThe function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\nbe able to choose different architectures according to hyper parameters (such as layer count, sizes of\ninner layers, dropout probabilities etc).\ncompute_metrics (Callable[[EvalPrediction], Dict], optional) —\nThe function that will be used to compute metrics at evaluation. Must take a EvalPrediction and return\na dictionary string to metric values.\ncallbacks (List of TrainerCallback, optional) —\nA list of callbacks to customize the training loop. Will add those to the list of default callbacks\ndetailed in here.\nIf you want to remove one of the default callbacks used, use the Trainer.remove_callback() method.\noptimizers (Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR], optional, defaults to (None, None)) —\nA tuple containing the optimizer and the scheduler to use. Will default to an instance of AdamW on your\nmodel and a scheduler given by get_linear_schedule_with_warmup() controlled by args.\npreprocess_logits_for_metrics (Callable[[torch.Tensor, torch.Tensor], torch.Tensor], optional) —\nA function that preprocess the logits right before caching them at each evaluation step. Must take two\ntensors, the logits and the labels, and return the logits once processed as desired. The modifications made\nby this function will be reflected in the predictions received by compute_metrics.\nNote that the labels (second parameter) will be None if the dataset does not have them.\nTrainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers. Important attributes: model — Always points to the core model. If using a transformers model, it will be a PreTrainedModel\nsubclass. model_wrapped — Always points to the most external model in case one or more other modules wrap the\noriginal model. This is the model that should be used for the forward pass. For example, under DeepSpeed,\nthe inner model is wrapped in DeepSpeed and then again in torch.nn.DistributedDataParallel. If the inner\nmodel hasn’t been wrapped, then self.model_wrapped is the same as self.model. is_model_parallel — Whether or not a model has been switched to a model parallel mode (different from\ndata parallelism, this means some of the model layers are split on different GPUs). place_model_on_device — Whether or not to automatically place the model on the device - it will be set\nto False if model parallel or deepspeed is used, or if the default\nTrainingArguments.place_model_on_device is overridden to return False . is_in_train — Whether or not a model is currently running train (e.g. when evaluate is called while\nin train) add_callback\n< source > ( callback )\nParameters\ncallback (type or ~transformer.TrainerCallback) —\nA ~transformer.TrainerCallback class or an instance of a ~transformer.TrainerCallback. In the\nfirst case, will instantiate a member of that class.\nAdd a callback to the current list of ~transformer.TrainerCallback. autocast_smart_context_manager\n< source > ( cache_enabled: typing.Optional[bool] = True )\nA helper wrapper that creates an appropriate context manager for autocast while feeding it the desired\narguments, depending on the situation. compute_loss\n< source > ( modelinputsreturn_outputs = False )\nHow the loss is computed by Trainer. By default, all models return the loss in the first element. Subclass and override for custom behavior. compute_loss_context_manager\n< source > ( )\nA helper wrapper to group together context managers. create_model_card\n< source > ( language: typing.Optional[str] = Nonelicense: typing.Optional[str] = Nonetags: typing.Union[str, typing.List[str], NoneType] = Nonemodel_name: typing.Optional[str] = Nonefinetuned_from: typing.Optional[str] = Nonetasks: typing.Union[str, typing.List[str], NoneType] = Nonedataset_tags: typing.Union[str, typing.List[str], NoneType] = Nonedataset: typing.Union[str, typing.List[str], NoneType] = Nonedataset_args: typing.Union[str, typing.List[str], NoneType] = None )\nExpand 9 parameters Parameters\nlanguage (str, optional) —\nThe language of the model (if applicable)\nlicense (str, optional) —\nThe license of the model. Will default to the license of the pretrained model used, if the original\nmodel given to the Trainer comes from a repo on the Hub.\ntags (str or List[str], optional) —\nSome tags to be included in the metadata of the model card.\nmodel_name (str, optional) —\nThe name of the model.\nfinetuned_from (str, optional) —\nThe name of the model used to fine-tune this one (if applicable). Will default to the name of the repo\nof the original model given to the Trainer (if it comes from the Hub).\ntasks (str or List[str], optional) —\nOne or several task identifiers, to be included in the metadata of the model card.\ndataset_tags (str or List[str], optional) —\nOne or several dataset tags, to be included in the metadata of the model card.\ndataset (str or List[str], optional) —\nOne or several dataset identifiers, to be included in the metadata of the model card.\ndataset_args (str or List[str], optional) —\nOne or several dataset arguments, to be included in the metadata of the model card.\nCreates a draft of a model card using the information available to the Trainer. create_optimizer\n< source > ( )\nSetup the optimizer. We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\nTrainer’s init through optimizers, or subclass and override this method in a subclass. create_optimizer_and_scheduler\n< source > ( num_training_steps: int )\nSetup the optimizer and the learning rate scheduler. We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\nTrainer’s init through optimizers, or subclass and override this method (or create_optimizer and/or\ncreate_scheduler) in a subclass. create_scheduler\n< source > ( num_training_steps: intoptimizer: Optimizer = None )\nParameters\nnum_training_steps (int) — The number of training steps to do.\nSetup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\npassed as an argument. evaluate\n< source > ( eval_dataset: typing.Optional[torch.utils.data.dataset.Dataset] = Noneignore_keys: typing.Optional[typing.List[str]] = Nonemetric_key_prefix: str = \'eval\' )\nParameters\neval_dataset (Dataset, optional) —\nPass a dataset if you wish to override self.eval_dataset. If it is a Dataset, columns\nnot accepted by the model.forward() method are automatically removed. It must implement the __len__\nmethod.\nignore_keys (List[str], optional) —\nA list of keys in the output of your model (if it is a dictionary) that should be ignored when\ngathering predictions.\nmetric_key_prefix (str, optional, defaults to "eval") —\nAn optional prefix to be used as the metrics key prefix. For example the metrics “bleu” will be named\n“eval_bleu” if the prefix is “eval” (default)\nRun evaluation and returns metrics. The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n(pass it to the init compute_metrics argument). You can also subclass and override this method to inject custom behavior. evaluation_loop\n< source > ( dataloader: DataLoaderdescription: strprediction_loss_only: typing.Optional[bool] = Noneignore_keys: typing.Optional[typing.List[str]] = Nonemetric_key_prefix: str = \'eval\' )\nPrediction/evaluation loop, shared by Trainer.evaluate() and Trainer.predict(). Works both with or without labels. floating_point_ops\n< source > ( inputs: typing.Dict[str, typing.Union[torch.Tensor, typing.Any]] ) → int\nParameters\ninputs (Dict[str, Union[torch.Tensor, Any]]) —\nThe inputs and targets of the model.\nReturns\nint\nThe number of floating-point operations.\nFor models that inherit from PreTrainedModel, uses that method to compute the number of floating point\noperations for every backward + forward pass. If using another model, either implement such a method in the\nmodel or subclass and override this method. get_decay_parameter_names\n< source > ( model )\nGet all parameter names that weight decay will be applied to Note that some models implement their own layernorm instead of calling nn.LayerNorm, weight decay could still\napply to those modules since this function only filter out instance of nn.LayerNorm get_eval_dataloader\n< source > ( eval_dataset: typing.Optional[torch.utils.data.dataset.Dataset] = None )\nParameters\neval_dataset (torch.utils.data.Dataset, optional) —\nIf provided, will override self.eval_dataset. If it is a Dataset, columns not accepted\nby the model.forward() method are automatically removed. It must implement __len__.\nReturns the evaluation ~torch.utils.data.DataLoader. Subclass and override this method if you want to inject some custom behavior. get_optimizer_cls_and_kwargs\n< source > ( args: TrainingArguments )\nParameters\nargs (transformers.training_args.TrainingArguments) —\nThe training arguments for the training session.\nReturns the optimizer class and optimizer parameters based on the training arguments. get_test_dataloader\n< source > ( test_dataset: Dataset )\nParameters\ntest_dataset (torch.utils.data.Dataset, optional) —\nThe test dataset to use. If it is a Dataset, columns not accepted by the\nmodel.forward() method are automatically removed. It must implement __len__.\nReturns the test ~torch.utils.data.DataLoader. Subclass and override this method if you want to inject some custom behavior. get_train_dataloader\n< source > ( )\nReturns the training ~torch.utils.data.DataLoader. Will use no sampler if train_dataset does not implement __len__, a random sampler (adapted to distributed\ntraining if necessary) otherwise. Subclass and override this method if you want to inject some custom behavior. hyperparameter_search\n< source > ( hp_space: typing.Union[typing.Callable[[ForwardRef(\'optuna.Trial\')], typing.Dict[str, float]], NoneType] = Nonecompute_objective: typing.Union[typing.Callable[[typing.Dict[str, float]], float], NoneType] = Nonen_trials: int = 20direction: typing.Union[str, typing.List[str]] = \'minimize\'backend: typing.Union[ForwardRef(\'str\'), transformers.trainer_utils.HPSearchBackend, NoneType] = Nonehp_name: typing.Union[typing.Callable[[ForwardRef(\'optuna.Trial\')], str], NoneType] = None**kwargs ) → [trainer_utils.BestRun or List[trainer_utils.BestRun]] Expand 7 parameters Parameters\nhp_space (Callable[["optuna.Trial"], Dict[str, float]], optional) —\nA function that defines the hyperparameter search space. Will default to\ndefault_hp_space_optuna() or default_hp_space_ray() or\ndefault_hp_space_sigopt() depending on your backend.\ncompute_objective (Callable[[Dict[str, float]], float], optional) —\nA function computing the objective to minimize or maximize from the metrics returned by the evaluate\nmethod. Will default to default_compute_objective().\nn_trials (int, optional, defaults to 100) —\nThe number of trial runs to test.\ndirection (str or List[str], optional, defaults to "minimize") —\nIf it’s single objective optimization, direction is str, can be "minimize" or "maximize", you\nshould pick "minimize" when optimizing the validation loss, "maximize" when optimizing one or\nseveral metrics. If it’s multi objectives optimization, direction is List[str], can be List of\n"minimize" and "maximize", you should pick "minimize" when optimizing the validation loss,\n"maximize" when optimizing one or several metrics.\nbackend (str or ~training_utils.HPSearchBackend, optional) —\nThe backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending\non which one is installed. If all are installed, will default to optuna.\nhp_name (Callable[["optuna.Trial"], str]], optional) —\nA function that defines the trial/run name. Will default to None.\nkwargs (Dict[str, Any], optional) —\nAdditional keyword arguments passed along to optuna.create_study or ray.tune.run. For more\ninformation see:\nthe documentation of\noptuna.create_study\nthe documentation of tune.run\nthe documentation of sigopt\nReturns\n[trainer_utils.BestRun or List[trainer_utils.BestRun]]\nAll the information about the best run or best\nruns for multi-objective optimization. Experiment summary can be found in run_summary attribute for Ray\nbackend.\nLaunch an hyperparameter search using optuna or Ray Tune or SigOpt. The optimized quantity is determined\nby compute_objective, which defaults to a function returning the evaluation loss when no metric is provided,\nthe sum of all metrics otherwise. To use this method, you need to have provided a model_init when initializing your Trainer: we need to\nreinitialize the model at each new run. This is incompatible with the optimizers argument, so you need to\nsubclass Trainer and override the method create_optimizer_and_scheduler() for custom\noptimizer/scheduler. init_git_repo\n< source > ( at_init: bool = False )\nParameters\nat_init (bool, optional, defaults to False) —\nWhether this function is called before any training or not. If self.args.overwrite_output_dir is\nTrue and at_init is True, the path to the repo (which is self.args.output_dir) might be wiped\nout.\nInitializes a git repo in self.args.hub_model_id. This function is deprecated and will be removed in v4.34.0 of Transformers. init_hf_repo\n< source > ( )\nInitializes a git repo in self.args.hub_model_id. is_local_process_zero\n< source > ( )\nWhether or not this process is the local (e.g., on one machine if training in a distributed fashion on several\nmachines) main process. is_world_process_zero\n< source > ( )\nWhether or not this process is the global main process (when training in a distributed fashion on several\nmachines, this is only going to be True for one process). log\n< source > ( logs: typing.Dict[str, float] )\nParameters\nlogs (Dict[str, float]) —\nThe values to log.\nLog logs on the various objects watching training. Subclass and override this method to inject custom behavior. log_metrics\n< source > ( splitmetrics )\nParameters\nsplit (str) —\nMode/split name: one of train, eval, test\nmetrics (Dict[str, float]) —\nThe metrics returned from train/evaluate/predictmetrics: metrics dict\nLog metrics in a specially formatted way Under distributed environment this is done only for a process with rank 0. Notes on memory reports: In order to get memory usage report you need to install psutil. You can do that with pip install psutil.\nNow when this method is run, you will see a report that will include: :\nCopied init_mem_cpu_alloc_delta\n=\n1301MB\ninit_mem_cpu_peaked_delta\n=\n154MB\ninit_mem_gpu_alloc_delta\n=\n230MB\ninit_mem_gpu_peaked_delta\n=\n0MB\ntrain_mem_cpu_alloc_delta\n=\n1345MB\ntrain_mem_cpu_peaked_delta =\n0MB\ntrain_mem_gpu_alloc_delta\n=\n693MB\ntrain_mem_gpu_peaked_delta =\n7MB Understanding the reports: the first segment, e.g., train__, tells you which stage the metrics are for. Reports starting with init_\nwill be added to the first stage that gets run. So that if only evaluation is run, the memory usage for the\n__init__ will be reported along with the eval_ metrics. the third segment, is either cpu or gpu, tells you whether it’s the general RAM or the gpu0 memory\nmetric. *_alloc_delta - is the difference in the used/allocated memory counter between the end and the start of the\nstage - it can be negative if a function released more memory than it allocated. *_peaked_delta - is any extra memory that was consumed and then freed - relative to the current allocated\nmemory counter - it is never negative. When you look at the metrics of any stage you add up alloc_delta +\npeaked_delta and you know how much memory was needed to complete that stage. The reporting happens only for process of rank 0 and gpu 0 (if there is a gpu). Typically this is enough since the\nmain process does the bulk of work, but it could be not quite so if model parallel is used and then other GPUs may\nuse a different amount of gpu memory. This is also not the same under DataParallel where gpu0 may require much more\nmemory than the rest since it stores the gradient and optimizer states for all participating GPUS. Perhaps in the\nfuture these reports will evolve to measure those too. The CPU RAM metric measures RSS (Resident Set Size) includes both the memory which is unique to the process and the\nmemory shared with other processes. It is important to note that it does not include swapped out memory, so the\nreports could be imprecise. The CPU peak memory is measured using a sampling thread. Due to python’s GIL it may miss some of the peak memory if\nthat thread didn’t get a chance to run when the highest memory was used. Therefore this report can be less than\nreality. Using tracemalloc would have reported the exact peak memory, but it doesn’t report memory allocations\noutside of python. So if some C++ CUDA extension allocated its own memory it won’t be reported. And therefore it\nwas dropped in favor of the memory sampling approach, which reads the current process memory usage. The GPU allocated and peak memory reporting is done with torch.cuda.memory_allocated() and\ntorch.cuda.max_memory_allocated(). This metric reports only “deltas” for pytorch-specific allocations, as\ntorch.cuda memory management system doesn’t track any memory allocated outside of pytorch. For example, the very\nfirst cuda call typically loads CUDA kernels, which may take from 0.5 to 2GB of GPU memory. Note that this tracker doesn’t account for memory allocations outside of Trainer’s __init__, train,\nevaluate and predict calls. Because evaluation calls may happen during train, we can’t handle nested invocations because\ntorch.cuda.max_memory_allocated is a single counter, so if it gets reset by a nested eval call, train’s tracker\nwill report incorrect info. If this pytorch issue gets resolved\nit will be possible to change this class to be re-entrant. Until then we will only track the outer level of\ntrain, evaluate and predict methods. Which means that if eval is called during train, it’s the latter\nthat will account for its memory usage and that of the former. This also means that if any other tool that is used along the Trainer calls\ntorch.cuda.reset_peak_memory_stats, the gpu peak memory stats could be invalid. And the Trainer will disrupt\nthe normal behavior of any such tools that rely on calling torch.cuda.reset_peak_memory_stats themselves. For best performance you may want to consider turning the memory profiling off for production runs. metrics_format\n< source > ( metrics: typing.Dict[str, float] ) → metrics (Dict[str, float])\nParameters\nmetrics (Dict[str, float]) —\nThe metrics returned from train/evaluate/predict\nReturns\nmetrics (Dict[str, float])\nThe reformatted metrics\nReformat Trainer metrics values to a human-readable format num_examples\n< source > ( dataloader: DataLoader )\nHelper to get number of samples in a ~torch.utils.data.DataLoader by accessing its dataset. When\ndataloader.dataset does not exist or has no length, estimates as best it can num_tokens\n< source > ( train_dl: DataLoadermax_steps: typing.Optional[int] = None )\nHelper to get number of tokens in a ~torch.utils.data.DataLoader by enumerating dataloader. pop_callback\n< source > ( callback ) → ~transformer.TrainerCallback\nParameters\ncallback (type or ~transformer.TrainerCallback) —\nA ~transformer.TrainerCallback class or an instance of a ~transformer.TrainerCallback. In the\nfirst case, will pop the first member of that class found in the list of callbacks.\nReturns\n~transformer.TrainerCallback\nThe callback removed, if found.\nRemove a callback from the current list of ~transformer.TrainerCallback and returns it. If the callback is not found, returns None (and no error is raised). predict\n< source > ( test_dataset: Datasetignore_keys: typing.Optional[typing.List[str]] = Nonemetric_key_prefix: str = \'test\' )\nParameters\ntest_dataset (Dataset) —\nDataset to run the predictions on. If it is an datasets.Dataset, columns not accepted by the\nmodel.forward() method are automatically removed. Has to implement the method __len__\nignore_keys (List[str], optional) —\nA list of keys in the output of your model (if it is a dictionary) that should be ignored when\ngathering predictions.\nmetric_key_prefix (str, optional, defaults to "test") —\nAn optional prefix to be used as the metrics key prefix. For example the metrics “bleu” will be named\n“test_bleu” if the prefix is “test” (default)\nRun prediction and returns predictions and potential metrics. Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method\nwill also return metrics, like in evaluate(). If your predictions or labels have different sequence length (for instance because you’re doing dynamic padding\nin a token classification task) the predictions will be padded (on the right) to allow for concatenation into\none array. The padding index is -100. Returns: NamedTuple A namedtuple with the following keys: predictions (np.ndarray): The predictions on test_dataset. label_ids (np.ndarray, optional): The labels (if the dataset contained some). metrics (Dict[str, float], optional): The potential dictionary of metrics (if the dataset contained\nlabels). prediction_loop\n< source > ( dataloader: DataLoaderdescription: strprediction_loss_only: typing.Optional[bool] = Noneignore_keys: typing.Optional[typing.List[str]] = Nonemetric_key_prefix: str = \'eval\' )\nPrediction/evaluation loop, shared by Trainer.evaluate() and Trainer.predict(). Works both with or without labels. prediction_step\n< source > ( model: Moduleinputs: typing.Dict[str, typing.Union[torch.Tensor, typing.Any]]prediction_loss_only: boolignore_keys: typing.Optional[typing.List[str]] = None ) → Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]\nParameters\nmodel (nn.Module) —\nThe model to evaluate.\ninputs (Dict[str, Union[torch.Tensor, Any]]) —\nThe inputs and targets of the model.\nThe dictionary will be unpacked before being fed to the model. Most models expect the targets under the\nargument labels. Check your model’s documentation for all accepted arguments.\nprediction_loss_only (bool) —\nWhether or not to return the loss only.\nignore_keys (List[str], optional) —\nA list of keys in the output of your model (if it is a dictionary) that should be ignored when\ngathering predictions.\nReturns\nTuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]\nA tuple with the loss,\nlogits and labels (each being optional).\nPerform an evaluation step on model using inputs. Subclass and override to inject custom behavior. push_to_hub\n< source > ( commit_message: typing.Optional[str] = \'End of training\'blocking: bool = True**kwargs )\nParameters\ncommit_message (str, optional, defaults to "End of training") —\nMessage to commit while pushing.\nblocking (bool, optional, defaults to True) —\nWhether the function should return only when the git push has finished.\nkwargs (Dict[str, Any], optional) —\nAdditional keyword arguments passed along to create_model_card().\nUpload self.model and self.tokenizer to the 🤗 model hub on the repo self.args.hub_model_id. remove_callback\n< source > ( callback )\nParameters\ncallback (type or ~transformer.TrainerCallback) —\nA ~transformer.TrainerCallback class or an instance of a ~transformer.TrainerCallback. In the\nfirst case, will remove the first member of that class found in the list of callbacks.\nRemove a callback from the current list of ~transformer.TrainerCallback. save_metrics\n< source > ( splitmetricscombined = True )\nParameters\nsplit (str) —\nMode/split name: one of train, eval, test, all\nmetrics (Dict[str, float]) —\nThe metrics returned from train/evaluate/predict\ncombined (bool, optional, defaults to True) —\nCreates combined metrics by updating all_results.json with metrics of this call\nSave metrics into a json file for that split, e.g. train_results.json. Under distributed environment this is done only for a process with rank 0. To understand the metrics please read the docstring of log_metrics(). The only difference is that raw\nunformatted numbers are saved in the current method. save_model\n< source > ( output_dir: typing.Optional[str] = None_internal_call: bool = False )\nWill save the model, so you can reload it using from_pretrained(). Will only save from the main process. save_state\n< source > ( )\nSaves the Trainer state, since Trainer.save_model saves only the tokenizer with the model Under distributed environment this is done only for a process with rank 0. train\n< source > ( resume_from_checkpoint: typing.Union[str, bool, NoneType] = Nonetrial: typing.Union[ForwardRef(\'optuna.Trial\'), typing.Dict[str, typing.Any]] = Noneignore_keys_for_eval: typing.Optional[typing.List[str]] = None**kwargs )\nParameters\nresume_from_checkpoint (str or bool, optional) —\nIf a str, local path to a saved checkpoint as saved by a previous instance of Trainer. If a\nbool and equals True, load the last checkpoint in args.output_dir as saved by a previous instance\nof Trainer. If present, training will resume from the model/optimizer/scheduler states loaded here.\ntrial (optuna.Trial or Dict[str, Any], optional) —\nThe trial run or the hyperparameter dictionary for hyperparameter search.\nignore_keys_for_eval (List[str], optional) —\nA list of keys in the output of your model (if it is a dictionary) that should be ignored when\ngathering predictions for evaluation during the training.\nkwargs (Dict[str, Any], optional) —\nAdditional keyword arguments used to hide deprecated arguments\nMain training entry point. training_step\n< source > ( model: Moduleinputs: typing.Dict[str, typing.Union[torch.Tensor, typing.Any]] ) → torch.Tensor\nParameters\nmodel (nn.Module) —\nThe model to train.\ninputs (Dict[str, Union[torch.Tensor, Any]]) —\nThe inputs and targets of the model.\nThe dictionary will be unpacked before being fed to the model. Most models expect the targets under the\nargument labels. Check your model’s documentation for all accepted arguments.\nReturns\ntorch.Tensor\nThe tensor with training loss on this batch.\nPerform a training step on a batch of inputs. Subclass and override to inject custom behavior.\nSeq2SeqTrainer class transformers.Seq2SeqTrainer\n< source > ( model: typing.Union[ForwardRef(\'PreTrainedModel\'), torch.nn.modules.module.Module] = Noneargs: TrainingArguments = Nonedata_collator: typing.Optional[ForwardRef(\'DataCollator\')] = Nonetrain_dataset: typing.Optional[torch.utils.data.dataset.Dataset] = Noneeval_dataset: typing.Union[torch.utils.data.dataset.Dataset, typing.Dict[str, torch.utils.data.dataset.Dataset], NoneType] = Nonetokenizer: typing.Optional[ForwardRef(\'PreTrainedTokenizerBase\')] = Nonemodel_init: typing.Union[typing.Callable[[], ForwardRef(\'PreTrainedModel\')], NoneType] = Nonecompute_metrics: typing.Union[typing.Callable[[ForwardRef(\'EvalPrediction\')], typing.Dict], NoneType] = Nonecallbacks: typing.Optional[typing.List[ForwardRef(\'TrainerCallback\')]] = Noneoptimizers: typing.Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None)preprocess_logits_for_metrics: typing.Union[typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor], NoneType] = None )\nevaluate\n< source > ( eval_dataset: typing.Optional[torch.utils.data.dataset.Dataset] = Noneignore_keys: typing.Optional[typing.List[str]] = Nonemetric_key_prefix: str = \'eval\'**gen_kwargs )\nParameters\neval_dataset (Dataset, optional) —\nPass a dataset if you wish to override self.eval_dataset. If it is an Dataset, columns\nnot accepted by the model.forward() method are automatically removed. It must implement the __len__\nmethod.\nignore_keys (List[str], optional) —\nA list of keys in the output of your model (if it is a dictionary) that should be ignored when\ngathering predictions.\nmetric_key_prefix (str, optional, defaults to "eval") —\nAn optional prefix to be used as the metrics key prefix. For example the metrics “bleu” will be named\n“eval_bleu” if the prefix is "eval" (default)\nmax_length (int, optional) —\nThe maximum target length to use when predicting with the generate method.\nnum_beams (int, optional) —\nNumber of beams for beam search that will be used when predicting with the generate method. 1 means no\nbeam search.\ngen_kwargs —\nAdditional generate specific kwargs.\nRun evaluation and returns metrics. The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n(pass it to the init compute_metrics argument). You can also subclass and override this method to inject custom behavior. predict\n< source > ( test_dataset: Datasetignore_keys: typing.Optional[typing.List[str]] = Nonemetric_key_prefix: str = \'test\'**gen_kwargs )\nParameters\ntest_dataset (Dataset) —\nDataset to run the predictions on. If it is a Dataset, columns not accepted by the\nmodel.forward() method are automatically removed. Has to implement the method __len__\nignore_keys (List[str], optional) —\nA list of keys in the output of your model (if it is a dictionary) that should be ignored when\ngathering predictions.\nmetric_key_prefix (str, optional, defaults to "eval") —\nAn optional prefix to be used as the metrics key prefix. For example the metrics “bleu” will be named\n“eval_bleu” if the prefix is "eval" (default)\nmax_length (int, optional) —\nThe maximum target length to use when predicting with the generate method.\nnum_beams (int, optional) —\nNumber of beams for beam search that will be used when predicting with the generate method. 1 means no\nbeam search.\ngen_kwargs —\nAdditional generate specific kwargs.\nRun prediction and returns predictions and potential metrics. Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method\nwill also return metrics, like in evaluate(). If your predictions or labels have different sequence lengths (for instance because you’re doing dynamic\npadding in a token classification task) the predictions will be padded (on the right) to allow for\nconcatenation into one array. The padding index is -100. Returns: NamedTuple A namedtuple with the following keys: predictions (np.ndarray): The predictions on test_dataset. label_ids (np.ndarray, optional): The labels (if the dataset contained some). metrics (Dict[str, float], optional): The potential dictionary of metrics (if the dataset contained\nlabels).\nTrainingArguments class transformers.TrainingArguments\n< source > ( output_dir: stroverwrite_output_dir: bool = Falsedo_train: bool = Falsedo_eval: bool = Falsedo_predict: bool = Falseevaluation_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = \'no\'prediction_loss_only: bool = Falseper_device_train_batch_size: int = 8per_device_eval_batch_size: int = 8per_gpu_train_batch_size: typing.Optional[int] = Noneper_gpu_eval_batch_size: typing.Optional[int] = Nonegradient_accumulation_steps: int = 1eval_accumulation_steps: typing.Optional[int] = Noneeval_delay: typing.Optional[float] = 0learning_rate: float = 5e-05weight_decay: float = 0.0adam_beta1: float = 0.9adam_beta2: float = 0.999adam_epsilon: float = 1e-08max_grad_norm: float = 1.0num_train_epochs: float = 3.0max_steps: int = -1lr_scheduler_type: typing.Union[transformers.trainer_utils.SchedulerType, str] = \'linear\'warmup_ratio: float = 0.0warmup_steps: int = 0log_level: typing.Optional[str] = \'passive\'log_level_replica: typing.Optional[str] = \'warning\'log_on_each_node: bool = Truelogging_dir: typing.Optional[str] = Nonelogging_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = \'steps\'logging_first_step: bool = Falselogging_steps: float = 500logging_nan_inf_filter: bool = Truesave_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = \'steps\'save_steps: float = 500save_total_limit: typing.Optional[int] = Nonesave_safetensors: typing.Optional[bool] = Truesave_on_each_node: bool = Falseno_cuda: bool = Falseuse_cpu: bool = Falseuse_mps_device: bool = Falseseed: int = 42data_seed: typing.Optional[int] = Nonejit_mode_eval: bool = Falseuse_ipex: bool = Falsebf16: bool = Falsefp16: bool = Falsefp16_opt_level: str = \'O1\'half_precision_backend: str = \'auto\'bf16_full_eval: bool = Falsefp16_full_eval: bool = Falsetf32: typing.Optional[bool] = Nonelocal_rank: int = -1ddp_backend: typing.Optional[str] = Nonetpu_num_cores: typing.Optional[int] = Nonetpu_metrics_debug: bool = Falsedebug: typing.Union[str, typing.List[transformers.debug_utils.DebugOption]] = \'\'dataloader_drop_last: bool = Falseeval_steps: typing.Optional[float] = Nonedataloader_num_workers: int = 0past_index: int = -1run_name: typing.Optional[str] = Nonedisable_tqdm: typing.Optional[bool] = Noneremove_unused_columns: typing.Optional[bool] = Truelabel_names: typing.Optional[typing.List[str]] = Noneload_best_model_at_end: typing.Optional[bool] = Falsemetric_for_best_model: typing.Optional[str] = Nonegreater_is_better: typing.Optional[bool] = Noneignore_data_skip: bool = Falsefsdp: typing.Union[typing.List[transformers.trainer_utils.FSDPOption], str, NoneType] = \'\'fsdp_min_num_params: int = 0fsdp_config: typing.Optional[str] = Nonefsdp_transformer_layer_cls_to_wrap: typing.Optional[str] = Nonedeepspeed: typing.Optional[str] = Nonelabel_smoothing_factor: float = 0.0optim: typing.Union[transformers.training_args.OptimizerNames, str] = \'adamw_torch\'optim_args: typing.Optional[str] = Noneadafactor: bool = Falsegroup_by_length: bool = Falselength_column_name: typing.Optional[str] = \'length\'report_to: typing.Optional[typing.List[str]] = Noneddp_find_unused_parameters: typing.Optional[bool] = Noneddp_bucket_cap_mb: typing.Optional[int] = Noneddp_broadcast_buffers: typing.Optional[bool] = Nonedataloader_pin_memory: bool = Trueskip_memory_metrics: bool = Trueuse_legacy_prediction_loop: bool = Falsepush_to_hub: bool = Falseresume_from_checkpoint: typing.Optional[str] = Nonehub_model_id: typing.Optional[str] = Nonehub_strategy: typing.Union[transformers.trainer_utils.HubStrategy, str] = \'every_save\'hub_token: typing.Optional[str] = Nonehub_private_repo: bool = Falsehub_always_push: bool = Falsegradient_checkpointing: bool = Falsegradient_checkpointing_kwargs: dict = Noneinclude_inputs_for_metrics: bool = Falsefp16_backend: str = \'auto\'push_to_hub_model_id: typing.Optional[str] = Nonepush_to_hub_organization: typing.Optional[str] = Nonepush_to_hub_token: typing.Optional[str] = Nonemp_parameters: str = \'\'auto_find_batch_size: bool = Falsefull_determinism: bool = Falsetorchdynamo: typing.Optional[str] = Noneray_scope: typing.Optional[str] = \'last\'ddp_timeout: typing.Optional[int] = 1800torch_compile: bool = Falsetorch_compile_backend: typing.Optional[str] = Nonetorch_compile_mode: typing.Optional[str] = Nonedispatch_batches: typing.Optional[bool] = Nonesplit_batches: typing.Optional[bool] = Falseinclude_tokens_per_second: typing.Optional[bool] = Falseneftune_noise_alpha: float = None )\nExpand 101 parameters Parameters\noutput_dir (str) —\nThe output directory where the model predictions and checkpoints will be written.\noverwrite_output_dir (bool, optional, defaults to False) —\nIf True, overwrite the content of the output directory. Use this to continue training if output_dir\npoints to a checkpoint directory.\ndo_train (bool, optional, defaults to False) —\nWhether to run training or not. This argument is not directly used by Trainer, it’s intended to be used\nby your training/evaluation scripts instead. See the example\nscripts for more details.\ndo_eval (bool, optional) —\nWhether to run evaluation on the validation set or not. Will be set to True if evaluation_strategy is\ndifferent from "no". This argument is not directly used by Trainer, it’s intended to be used by your\ntraining/evaluation scripts instead. See the example\nscripts for more details.\ndo_predict (bool, optional, defaults to False) —\nWhether to run predictions on the test set or not. This argument is not directly used by Trainer, it’s\nintended to be used by your training/evaluation scripts instead. See the example\nscripts for more details.\nevaluation_strategy (str or IntervalStrategy, optional, defaults to "no") —\nThe evaluation strategy to adopt during training. Possible values are:\n"no": No evaluation is done during training.\n"steps": Evaluation is done (and logged) every eval_steps.\n"epoch": Evaluation is done at the end of each epoch.\nprediction_loss_only (bool, optional, defaults to False) —\nWhen performing evaluation and generating predictions, only returns the loss.\nper_device_train_batch_size (int, optional, defaults to 8) —\nThe batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training.\nper_device_eval_batch_size (int, optional, defaults to 8) —\nThe batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.\ngradient_accumulation_steps (int, optional, defaults to 1) —\nNumber of updates steps to accumulate the gradients for, before performing a backward/update pass.\nWhen using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,\nevaluation, save will be conducted every gradient_accumulation_steps * xxx_step training examples.\neval_accumulation_steps (int, optional) —\nNumber of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\nleft unset, the whole predictions are accumulated on GPU/NPU/TPU before being moved to the CPU (faster but\nrequires more memory).\neval_delay (float, optional) —\nNumber of epochs or steps to wait for before the first evaluation can be performed, depending on the\nevaluation_strategy.\nlearning_rate (float, optional, defaults to 5e-5) —\nThe initial learning rate for AdamW optimizer.\nweight_decay (float, optional, defaults to 0) —\nThe weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW\noptimizer.\nadam_beta1 (float, optional, defaults to 0.9) —\nThe beta1 hyperparameter for the AdamW optimizer.\nadam_beta2 (float, optional, defaults to 0.999) —\nThe beta2 hyperparameter for the AdamW optimizer.\nadam_epsilon (float, optional, defaults to 1e-8) —\nThe epsilon hyperparameter for the AdamW optimizer.\nmax_grad_norm (float, optional, defaults to 1.0) —\nMaximum gradient norm (for gradient clipping).\nnum_train_epochs(float, optional, defaults to 3.0) —\nTotal number of training epochs to perform (if not an integer, will perform the decimal part percents of\nthe last epoch before stopping training).\nmax_steps (int, optional, defaults to -1) —\nIf set to a positive number, the total number of training steps to perform. Overrides num_train_epochs.\nIn case of using a finite iterable dataset the training may stop before reaching the set number of steps\nwhen all data is exhausted\nlr_scheduler_type (str or SchedulerType, optional, defaults to "linear") —\nThe scheduler type to use. See the documentation of SchedulerType for all possible values.\nwarmup_ratio (float, optional, defaults to 0.0) —\nRatio of total training steps used for a linear warmup from 0 to learning_rate.\nwarmup_steps (int, optional, defaults to 0) —\nNumber of steps used for a linear warmup from 0 to learning_rate. Overrides any effect of warmup_ratio.\nlog_level (str, optional, defaults to passive) —\nLogger log level to use on the main process. Possible choices are the log levels as strings: ‘debug’,\n‘info’, ‘warning’, ‘error’ and ‘critical’, plus a ‘passive’ level which doesn’t set anything and keeps the\ncurrent log level for the Transformers library (which will be "warning" by default).\nlog_level_replica (str, optional, defaults to "warning") —\nLogger log level to use on replicas. Same choices as log_level”\nlog_on_each_node (bool, optional, defaults to True) —\nIn multinode distributed training, whether to log using log_level once per node, or only on the main\nnode.\nlogging_dir (str, optional) —\nTensorBoard log directory. Will default to\n*output_dir/runs/CURRENT_DATETIME_HOSTNAME*.\nlogging_strategy (str or IntervalStrategy, optional, defaults to "steps") —\nThe logging strategy to adopt during training. Possible values are:\n"no": No logging is done during training.\n"epoch": Logging is done at the end of each epoch.\n"steps": Logging is done every logging_steps.\nlogging_first_step (bool, optional, defaults to False) —\nWhether to log and evaluate the first global_step or not.\nlogging_steps (int or float, optional, defaults to 500) —\nNumber of update steps between two logs if logging_strategy="steps". Should be an integer or a float in\nrange [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\nlogging_nan_inf_filter (bool, optional, defaults to True) —\nWhether to filter nan and inf losses for logging. If set to True the loss of every step that is nan\nor inf is filtered and the average loss of the current logging window is taken instead.\nlogging_nan_inf_filter only influences the logging of loss values, it does not change the behavior the\ngradient is computed or applied to the model.\nsave_strategy (str or IntervalStrategy, optional, defaults to "steps") —\nThe checkpoint save strategy to adopt during training. Possible values are:\n"no": No save is done during training.\n"epoch": Save is done at the end of each epoch.\n"steps": Save is done every save_steps.\nsave_steps (int or float, optional, defaults to 500) —\nNumber of updates steps before two checkpoint saves if save_strategy="steps". Should be an integer or a\nfloat in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\nsave_total_limit (int, optional) —\nIf a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\noutput_dir. When load_best_model_at_end is enabled, the “best” checkpoint according to\nmetric_for_best_model will always be retained in addition to the most recent ones. For example, for\nsave_total_limit=5 and load_best_model_at_end, the four last checkpoints will always be retained\nalongside the best model. When save_total_limit=1 and load_best_model_at_end, it is possible that two\ncheckpoints are saved: the last one and the best one (if they are different).\nsave_safetensors (bool, optional, defaults to True) —\nUse safetensors saving and loading for state dicts instead of\ndefault torch.load and torch.save.\nsave_on_each_node (bool, optional, defaults to False) —\nWhen doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\nthe main one.\nThis should not be activated when the different nodes use the same storage as the files will be saved with\nthe same names for each node.\nuse_cpu (bool, optional, defaults to False) —\nWhether or not to use cpu. If set to False, we will use cuda or mps device if available.\nseed (int, optional, defaults to 42) —\nRandom seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n~Trainer.model_init function to instantiate the model if it has some randomly initialized parameters.\ndata_seed (int, optional) —\nRandom seed to be used with data samplers. If not set, random generators for data sampling will use the\nsame seed as seed. This can be used to ensure reproducibility of data sampling, independent of the model\nseed.\njit_mode_eval (bool, optional, defaults to False) —\nWhether or not to use PyTorch jit trace for inference.\nuse_ipex (bool, optional, defaults to False) —\nUse Intel extension for PyTorch when it is available. IPEX\ninstallation.\nbf16 (bool, optional, defaults to False) —\nWhether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\nNVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\nfp16 (bool, optional, defaults to False) —\nWhether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\nfp16_opt_level (str, optional, defaults to ‘O1’) —\nFor fp16 training, Apex AMP optimization level selected in [‘O0’, ‘O1’, ‘O2’, and ‘O3’]. See details on\nthe Apex documentation.\nfp16_backend (str, optional, defaults to "auto") —\nThis argument is deprecated. Use half_precision_backend instead.\nhalf_precision_backend (str, optional, defaults to "auto") —\nThe backend to use for mixed precision training. Must be one of "auto", "apex", "cpu_amp". "auto" will\nuse CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the\nrequested backend.\nbf16_full_eval (bool, optional, defaults to False) —\nWhether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\nmetric values. This is an experimental API and it may change.\nfp16_full_eval (bool, optional, defaults to False) —\nWhether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\nmetric values.\ntf32 (bool, optional) —\nWhether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\non PyTorch’s version default of torch.backends.cuda.matmul.allow_tf32. For more details please refer to\nthe TF32 documentation. This is an\nexperimental API and it may change.\nlocal_rank (int, optional, defaults to -1) —\nRank of the process during distributed training.\nddp_backend (str, optional) —\nThe backend to use for distributed training. Must be one of "nccl", "mpi", "ccl", "gloo", "hccl".\ntpu_num_cores (int, optional) —\nWhen training on TPU, the number of TPU cores (automatically passed by launcher script).\ndataloader_drop_last (bool, optional, defaults to False) —\nWhether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\nor not.\neval_steps (int or float, optional) —\nNumber of update steps between two evaluations if evaluation_strategy="steps". Will default to the same\nvalue as logging_steps if not set. Should be an integer or a float in range [0,1). If smaller than 1,\nwill be interpreted as ratio of total training steps.\ndataloader_num_workers (int, optional, defaults to 0) —\nNumber of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\nmain process.\npast_index (int, optional, defaults to -1) —\nSome models like TransformerXL or XLNet can make use of\nthe past hidden states for their predictions. If this argument is set to a positive int, the Trainer will\nuse the corresponding output (usually index 2) as the past state and feed it to the model at the next\ntraining step under the keyword argument mems.\nrun_name (str, optional) —\nA descriptor for the run. Typically used for wandb and\nmlflow logging.\ndisable_tqdm (bool, optional) —\nWhether or not to disable the tqdm progress bars and table of metrics produced by\n~notebook.NotebookTrainingTracker in Jupyter Notebooks. Will default to True if the logging level is\nset to warn or lower (default), False otherwise.\nremove_unused_columns (bool, optional, defaults to True) —\nWhether or not to automatically remove the columns unused by the model forward method.\n(Note that this behavior is not implemented for TFTrainer yet.)\nlabel_names (List[str], optional) —\nThe list of keys in your dictionary of inputs that correspond to the labels.\nWill eventually default to the list of argument names accepted by the model that contain the word “label”,\nexcept if the model used is one of the XxxForQuestionAnswering in which case it will also include the\n["start_positions", "end_positions"] keys.\nload_best_model_at_end (bool, optional, defaults to False) —\nWhether or not to load the best model found during training at the end of training. When this option is\nenabled, the best checkpoint will always be saved. See\nsave_total_limit\nfor more.\nWhen set to True, the parameters save_strategy needs to be the same as evaluation_strategy, and in\nthe case it is “steps”, save_steps must be a round multiple of eval_steps.\nmetric_for_best_model (str, optional) —\nUse in conjunction with load_best_model_at_end to specify the metric to use to compare two different\nmodels. Must be the name of a metric returned by the evaluation with or without the prefix "eval_". Will\ndefault to "loss" if unspecified and load_best_model_at_end=True (to use the evaluation loss).\nIf you set this value, greater_is_better will default to True. Don’t forget to set it to False if\nyour metric is better when lower.\ngreater_is_better (bool, optional) —\nUse in conjunction with load_best_model_at_end and metric_for_best_model to specify if better models\nshould have a greater metric or not. Will default to:\nTrue if metric_for_best_model is set to a value that isn’t "loss" or "eval_loss".\nFalse if metric_for_best_model is not set, or set to "loss" or "eval_loss".\nignore_data_skip (bool, optional, defaults to False) —\nWhen resuming training, whether or not to skip the epochs and batches to get the data loading at the same\nstage as in the previous training. If set to True, the training will begin faster (as that skipping step\ncan take a long time) but will not yield the same results as the interrupted training would have.\nfsdp (bool, str or list of FSDPOption, optional, defaults to \'\') —\nUse PyTorch Distributed Parallel Training (in distributed training only).\nA list of options along the following:\n"full_shard": Shard parameters, gradients and optimizer states.\n"shard_grad_op": Shard optimizer states and gradients.\n"offload": Offload parameters and gradients to CPUs (only compatible with "full_shard" and\n"shard_grad_op").\n"auto_wrap": Automatically recursively wrap layers with FSDP using default_auto_wrap_policy.\nfsdp_config (str or dict, optional) —\nConfig to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location of\ndeepspeed json config file (e.g., ds_config.json) or an already loaded json file as dict.\nA List of config and its options:\nmin_num_params (int, optional, defaults to 0):\nFSDP’s minimum number of parameters for Default Auto Wrapping. (useful only when fsdp field is\npassed).\ntransformer_layer_cls_to_wrap (List[str], optional):\nList of transformer layer class names (case-sensitive) to wrap, e.g, BertLayer, GPTJBlock,\nT5Block … (useful only when fsdp flag is passed).\nbackward_prefetch (str, optional)\nFSDP’s backward prefetch mode. Controls when to prefetch next set of parameters (useful only when\nfsdp field is passed).\nA list of options along the following:\n"backward_pre" : Prefetches the next set of parameters before the current set of parameter’s\ngradient\ncomputation.\n"backward_post" : This prefetches the next set of parameters after the current set of\nparameter’s\ngradient computation.\nforward_prefetch (bool, optional, defaults to False)\nFSDP’s forward prefetch mode (useful only when fsdp field is passed).\nIf "True", then FSDP explicitly prefetches the next upcoming all-gather while executing in the\nforward pass.\nlimit_all_gathers (bool, optional, defaults to False)\nFSDP’s limit_all_gathers (useful only when fsdp field is passed).\nIf "True", FSDP explicitly synchronizes the CPU thread to prevent too many in-flight\nall-gathers.\nuse_orig_params (bool, optional, defaults to False)\nIf "True", allows non-uniform requires_grad during init, which means support for interspersed\nfrozen and trainable paramteres. Useful in cases such as parameter-efficient fine-tuning. Please\nrefer this\n[blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019\nsync_module_states (bool, optional, defaults to True)\nIf "True", each individually wrapped FSDP unit will broadcast module parameters from rank 0 to\nensure they are the same across all ranks after initialization\nxla (bool, optional, defaults to False):\nWhether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental feature\nand its API may evolve in the future.\nxla_fsdp_settings (dict, optional)\nThe value is a dictionary which stores the XLA FSDP wrapping parameters.\nFor a complete list of options, please see here.\nxla_fsdp_grad_ckpt (bool, optional, defaults to False):\nWill use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be\nused when the xla flag is set to true, and an auto wrapping policy is specified through\nfsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.\nactivation_checkpointing (bool, optional, defaults to False):\nIf True, activation checkpointing is a technique to reduce memory usage by clearing activations of\ncertain layers and recomputing them during a backward pass. Effectively, this trades extra\ncomputation time for reduced memory usage.\ndeepspeed (str or dict, optional) —\nUse Deepspeed. This is an experimental feature and its API may\nevolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\nds_config.json) or an already loaded json file as a dict”\nlabel_smoothing_factor (float, optional, defaults to 0.0) —\nThe label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\nlabels are changed from 0s and 1s to label_smoothing_factor/num_labels and 1 - label_smoothing_factor + label_smoothing_factor/num_labels respectively.\ndebug (str or list of DebugOption, optional, defaults to "") —\nEnable one or more debug features. This is an experimental feature.\nPossible options are:\n"underflow_overflow": detects overflow in model’s input/outputs and reports the last frames that led to\nthe event\n"tpu_metrics_debug": print debug metrics on TPU\nThe options should be separated by whitespaces.\noptim (str or training_args.OptimizerNames, optional, defaults to "adamw_torch") —\nThe optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or\nadafactor.\noptim_args (str, optional) —\nOptional arguments that are supplied to AnyPrecisionAdamW.\ngroup_by_length (bool, optional, defaults to False) —\nWhether or not to group together samples of roughly the same length in the training dataset (to minimize\npadding applied and be more efficient). Only useful if applying dynamic padding.\nlength_column_name (str, optional, defaults to "length") —\nColumn name for precomputed lengths. If the column exists, grouping by length will use these values rather\nthan computing them on train startup. Ignored unless group_by_length is True and the dataset is an\ninstance of Dataset.\nreport_to (str or List[str], optional, defaults to "all") —\nThe list of integrations to report the results and logs to. Supported platforms are "azure_ml",\n"clearml", "codecarbon", "comet_ml", "dagshub", "flyte", "mlflow", "neptune",\n"tensorboard", and "wandb". Use "all" to report to all integrations installed, "none" for no\nintegrations.\nddp_find_unused_parameters (bool, optional) —\nWhen using distributed training, the value of the flag find_unused_parameters passed to\nDistributedDataParallel. Will default to False if gradient checkpointing is used, True otherwise.\nddp_bucket_cap_mb (int, optional) —\nWhen using distributed training, the value of the flag bucket_cap_mb passed to DistributedDataParallel.\nddp_broadcast_buffers (bool, optional) —\nWhen using distributed training, the value of the flag broadcast_buffers passed to\nDistributedDataParallel. Will default to False if gradient checkpointing is used, True otherwise.\ndataloader_pin_memory (bool, optional, defaults to True) —\nWhether you want to pin memory in data loaders or not. Will default to True.\nskip_memory_metrics (bool, optional, defaults to True) —\nWhether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\ndown the training and evaluation speed.\npush_to_hub (bool, optional, defaults to False) —\nWhether or not to push the model to the Hub every time the model is saved. If this is activated,\noutput_dir will begin a git directory synced with the repo (determined by hub_model_id) and the content\nwill be pushed each time a save is triggered (depending on your save_strategy). Calling\nsave_model() will also trigger a push.\nIf output_dir exists, it needs to be a local clone of the repository to which the Trainer will be\npushed.\nresume_from_checkpoint (str, optional) —\nThe path to a folder with a valid checkpoint for your model. This argument is not directly used by\nTrainer, it’s intended to be used by your training/evaluation scripts instead. See the example\nscripts for more details.\nhub_model_id (str, optional) —\nThe name of the repository to keep in sync with the local output_dir. It can be a simple model ID in\nwhich case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\nfor instance "user_name/model", which allows you to push to an organization you are a member of with\n"organization_name/model". Will default to user_name/output_dir_name with output_dir_name being the\nname of output_dir.\nWill default to the name of output_dir.\nhub_strategy (str or HubStrategy, optional, defaults to "every_save") —\nDefines the scope of what is pushed to the Hub and when. Possible values are:\n"end": push the model, its configuration, the tokenizer (if passed along to the Trainer) and a\ndraft of a model card when the save_model() method is called.\n"every_save": push the model, its configuration, the tokenizer (if passed along to the Trainer) and\na draft of a model card each time there is a model save. The pushes are asynchronous to not block\ntraining, and in case the save are very frequent, a new push is only attempted if the previous one is\nfinished. A last push is made with the final model at the end of training.\n"checkpoint": like "every_save" but the latest checkpoint is also pushed in a subfolder named\nlast-checkpoint, allowing you to resume training easily with\ntrainer.train(resume_from_checkpoint="last-checkpoint").\n"all_checkpoints": like "checkpoint" but all checkpoints are pushed like they appear in the output\nfolder (so you will get one checkpoint folder per folder in your final repository)\nhub_token (str, optional) —\nThe token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\nhuggingface-cli login.\nhub_private_repo (bool, optional, defaults to False) —\nIf True, the Hub repo will be set to private.\nhub_always_push (bool, optional, defaults to False) —\nUnless this is True, the Trainer will skip pushing a checkpoint when the previous push is not finished.\ngradient_checkpointing (bool, optional, defaults to False) —\nIf True, use gradient checkpointing to save memory at the expense of slower backward pass.\ngradient_checkpointing_args (dict, optional, defaults to None) —\nKey word arguments to be passed to the gradient_checkpointing_enable method.\ninclude_inputs_for_metrics (bool, optional, defaults to False) —\nWhether or not the inputs will be passed to the compute_metrics function. This is intended for metrics\nthat need inputs, predictions and references for scoring calculation in Metric class.\nauto_find_batch_size (bool, optional, defaults to False) —\nWhether to find a batch size that will fit into memory automatically through exponential decay, avoiding\nCUDA Out-of-Memory errors. Requires accelerate to be installed (pip install accelerate)\nfull_determinism (bool, optional, defaults to False) —\nIf True, enable_full_determinism() is called instead of set_seed() to ensure reproducible results in\ndistributed training. Important: this will negatively impact the performance, so only use it for debugging.\ntorchdynamo (str, optional) —\nIf set, the backend compiler for TorchDynamo. Possible choices are "eager", "aot_eager", "inductor",\n"nvfuser", "aot_nvfuser", "aot_cudagraphs", "ofi", "fx2trt", "onnxrt" and "ipex".\nray_scope (str, optional, defaults to "last") —\nThe scope to use when doing hyperparameter search with Ray. By default, "last" will be used. Ray will\nthen use the last checkpoint of all trials, compare those, and select the best one. However, other options\nare also available. See the Ray documentation for\nmore options.\nddp_timeout (int, optional, defaults to 1800) —\nThe timeout for torch.distributed.init_process_group calls, used to avoid GPU socket timeouts when\nperforming slow operations in distributed runnings. Please refer the [PyTorch documentation]\n(https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more\ninformation.\nuse_mps_device (bool, optional, defaults to False) —\nThis argument is deprecated.mps device will be used if it is available similar to cuda device.\ntorch_compile (bool, optional, defaults to False) —\nWhether or not to compile the model using PyTorch 2.0\ntorch.compile.\nThis will use the best defaults for the torch.compile\nAPI.\nYou can customize the defaults with the argument torch_compile_backend and torch_compile_mode but we\ndon’t guarantee any of them will work as the support is progressively rolled in in PyTorch.\nThis flag and the whole compile API is experimental and subject to change in future releases.\ntorch_compile_backend (str, optional) —\nThe backend to use in torch.compile. If set to any value, torch_compile will be set to True.\nRefer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\nThis flag is experimental and subject to change in future releases.\ntorch_compile_mode (str, optional) —\nThe mode to use in torch.compile. If set to any value, torch_compile will be set to True.\nRefer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\nThis flag is experimental and subject to change in future releases.\nsplit_batches (bool, optional) —\nWhether or not the accelerator should split the batches yielded by the dataloaders across the devices\nduring distributed training. If\nset to True, the actual batch size used will be the same on any kind of distributed processes, but it\nmust be a\nround multiple of the number of processes you are using (such as GPUs).\ninclude_tokens_per_second (bool, optional) —\nWhether or not to compute the number of tokens per second per device for training speed metrics.\nThis will iterate over the entire training dataloader once beforehand,\nand will slow down the entire process.\nneftune_noise_alpha (Optional[float]) —\nIf not None, this will activate NEFTune noise embeddings. This can drastically improve model performance\nfor instruction fine-tuning. Check out the original paper and the\noriginal code. Support transformers PreTrainedModel and also\nPeftModel from peft.\nTrainingArguments is the subset of the arguments we use in our example scripts which relate to the training loop\nitself. Using HfArgumentParser we can turn this class into\nargparse arguments that can be specified on the\ncommand line. get_process_log_level\n< source > ( )\nReturns the log level to be used depending on whether this process is the main process of node 0, main process\nof node non-0, or a non-main process. For the main process the log level defaults to the logging level set (logging.WARNING if you didn’t do\nanything) unless overridden by log_level argument. For the replica processes the log level defaults to logging.WARNING unless overridden by log_level_replica\nargument. The choice between the main and replica process settings is made according to the return value of should_log. get_warmup_steps\n< source > ( num_training_steps: int )\nGet number of steps used for a linear warmup. main_process_first\n< source > ( local = Truedesc = \'work\' )\nParameters\nlocal (bool, optional, defaults to True) —\nif True first means process of rank 0 of each node if False first means process of rank 0 of node\nrank 0 In multi-node environment with a shared filesystem you most likely will want to use\nlocal=False so that only the main process of the first node will do the processing. If however, the\nfilesystem is not shared, then the main process of each node will need to do the processing, which is\nthe default behavior.\ndesc (str, optional, defaults to "work") —\na work description to be used in debug logs\nA context manager for torch distributed environment where on needs to do something on the main process, while\nblocking replicas, and when it’s finished releasing the replicas. One such use is for datasets’s map feature which to be efficient should be run once on the main process,\nwhich upon completion saves a cached version of results and which then automatically gets loaded by the\nreplicas. set_dataloader\n< source > ( train_batch_size: int = 8eval_batch_size: int = 8drop_last: bool = Falsenum_workers: int = 0pin_memory: bool = Trueauto_find_batch_size: bool = Falseignore_data_skip: bool = Falsesampler_seed: typing.Optional[int] = None )\nExpand 6 parameters Parameters\ndrop_last (bool, optional, defaults to False) —\nWhether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch\nsize) or not.\nnum_workers (int, optional, defaults to 0) —\nNumber of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in\nthe main process.\npin_memory (bool, optional, defaults to True) —\nWhether you want to pin memory in data loaders or not. Will default to True.\nauto_find_batch_size (bool, optional, defaults to False) —\nWhether to find a batch size that will fit into memory automatically through exponential decay,\navoiding CUDA Out-of-Memory errors. Requires accelerate to be installed (pip install accelerate)\nignore_data_skip (bool, optional, defaults to False) —\nWhen resuming training, whether or not to skip the epochs and batches to get the data loading at the\nsame stage as in the previous training. If set to True, the training will begin faster (as that\nskipping step can take a long time) but will not yield the same results as the interrupted training\nwould have.\nsampler_seed (int, optional) —\nRandom seed to be used with data samplers. If not set, random generators for data sampling will use the\nsame seed as self.seed. This can be used to ensure reproducibility of data sampling, independent of\nthe model seed.\nA method that regroups all arguments linked to the dataloaders creation.\nExample:\nCopied >>> from transformers import TrainingArguments\n>>> args = TrainingArguments("working_dir")\n>>> args = args.set_dataloader(train_batch_size=16, eval_batch_size=64)\n>>> args.per_device_train_batch_size\n16 set_evaluate\n< source > ( strategy: typing.Union[str, transformers.trainer_utils.IntervalStrategy] = \'no\'steps: int = 500batch_size: int = 8accumulation_steps: typing.Optional[int] = Nonedelay: typing.Optional[float] = Noneloss_only: bool = Falsejit_mode: bool = False )\nExpand 7 parameters Parameters\nstrategy (str or IntervalStrategy, optional, defaults to "no") —\nThe evaluation strategy to adopt during training. Possible values are:\n"no": No evaluation is done during training.\n"steps": Evaluation is done (and logged) every steps.\n"epoch": Evaluation is done at the end of each epoch.\nSetting a strategy different from "no" will set self.do_eval to True.\nsteps (int, optional, defaults to 500) —\nNumber of update steps between two evaluations if strategy="steps".\nbatch_size (int optional, defaults to 8) —\nThe batch size per device (GPU/TPU core/CPU…) used for evaluation.\naccumulation_steps (int, optional) —\nNumber of predictions steps to accumulate the output tensors for, before moving the results to the CPU.\nIf left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster\nbut requires more memory).\ndelay (float, optional) —\nNumber of epochs or steps to wait for before the first evaluation can be performed, depending on the\nevaluation_strategy.\nloss_only (bool, optional, defaults to False) —\nIgnores all outputs except the loss.\njit_mode (bool, optional) —\nWhether or not to use PyTorch jit trace for inference.\nA method that regroups all arguments linked to evaluation.\nExample:\nCopied >>> from transformers import TrainingArguments\n>>> args = TrainingArguments("working_dir")\n>>> args = args.set_evaluate(strategy="steps", steps=100)\n>>> args.eval_steps\n100 set_logging\n< source > ( strategy: typing.Union[str, transformers.trainer_utils.IntervalStrategy] = \'steps\'steps: int = 500report_to: typing.Union[str, typing.List[str]] = \'none\'level: str = \'passive\'first_step: bool = Falsenan_inf_filter: bool = Falseon_each_node: bool = Falsereplica_level: str = \'passive\' )\nExpand 8 parameters Parameters\nstrategy (str or IntervalStrategy, optional, defaults to "steps") —\nThe logging strategy to adopt during training. Possible values are:\n"no": No save is done during training.\n"epoch": Save is done at the end of each epoch.\n"steps": Save is done every save_steps.\nsteps (int, optional, defaults to 500) —\nNumber of update steps between two logs if strategy="steps".\nlevel (str, optional, defaults to "passive") —\nLogger log level to use on the main process. Possible choices are the log levels as strings: "debug",\n"info", "warning", "error" and "critical", plus a "passive" level which doesn’t set anything\nand lets the application set the level.\nreport_to (str or List[str], optional, defaults to "all") —\nThe list of integrations to report the results and logs to. Supported platforms are "azure_ml",\n"clearml", "codecarbon", "comet_ml", "dagshub", "flyte", "mlflow", "neptune",\n"tensorboard", and "wandb". Use "all" to report to all integrations installed, "none" for no\nintegrations.\nfirst_step (bool, optional, defaults to False) —\nWhether to log and evaluate the first global_step or not.\nnan_inf_filter (bool, optional, defaults to True) —\nWhether to filter nan and inf losses for logging. If set to True the loss of every step that is\nnan or inf is filtered and the average loss of the current logging window is taken instead.\nnan_inf_filter only influences the logging of loss values, it does not change the behavior the\ngradient is computed or applied to the model.\non_each_node (bool, optional, defaults to True) —\nIn multinode distributed training, whether to log using log_level once per node, or only on the main\nnode.\nreplica_level (str, optional, defaults to "passive") —\nLogger log level to use on replicas. Same choices as log_level\nA method that regroups all arguments linked to logging.\nExample:\nCopied >>> from transformers import TrainingArguments\n>>> args = TrainingArguments("working_dir")\n>>> args = args.set_logging(strategy="steps", steps=100)\n>>> args.logging_steps\n100 set_lr_scheduler\n< source > ( name: typing.Union[str, transformers.trainer_utils.SchedulerType] = \'linear\'num_epochs: float = 3.0max_steps: int = -1warmup_ratio: float = 0warmup_steps: int = 0 )\nParameters\nname (str or SchedulerType, optional, defaults to "linear") —\nThe scheduler type to use. See the documentation of SchedulerType for all possible values.\nnum_epochs(float, optional, defaults to 3.0) —\nTotal number of training epochs to perform (if not an integer, will perform the decimal part percents\nof the last epoch before stopping training).\nmax_steps (int, optional, defaults to -1) —\nIf set to a positive number, the total number of training steps to perform. Overrides\nnum_train_epochs. In case of using a finite iterable dataset the training may stop before reaching\nthe set number of steps when all data is exhausted.\nwarmup_ratio (float, optional, defaults to 0.0) —\nRatio of total training steps used for a linear warmup from 0 to learning_rate.\nwarmup_steps (int, optional, defaults to 0) —\nNumber of steps used for a linear warmup from 0 to learning_rate. Overrides any effect of\nwarmup_ratio.\nA method that regroups all arguments linked to the learning rate scheduler and its hyperparameters.\nExample:\nCopied >>> from transformers import TrainingArguments\n>>> args = TrainingArguments("working_dir")\n>>> args = args.set_lr_scheduler(name="cosine", warmup_ratio=0.05)\n>>> args.warmup_ratio\n0.05 set_optimizer\n< source > ( name: typing.Union[str, transformers.training_args.OptimizerNames] = \'adamw_torch\'learning_rate: float = 5e-05weight_decay: float = 0beta1: float = 0.9beta2: float = 0.999epsilon: float = 1e-08args: typing.Optional[str] = None )\nParameters\nname (str or training_args.OptimizerNames, optional, defaults to "adamw_torch") —\nThe optimizer to use: "adamw_hf", "adamw_torch", "adamw_torch_fused", "adamw_apex_fused",\n"adamw_anyprecision" or "adafactor".\nlearning_rate (float, optional, defaults to 5e-5) —\nThe initial learning rate.\nweight_decay (float, optional, defaults to 0) —\nThe weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights.\nbeta1 (float, optional, defaults to 0.9) —\nThe beta1 hyperparameter for the adam optimizer or its variants.\nbeta2 (float, optional, defaults to 0.999) —\nThe beta2 hyperparameter for the adam optimizer or its variants.\nepsilon (float, optional, defaults to 1e-8) —\nThe epsilon hyperparameter for the adam optimizer or its variants.\nargs (str, optional) —\nOptional arguments that are supplied to AnyPrecisionAdamW (only useful when\noptim="adamw_anyprecision").\nA method that regroups all arguments linked to the optimizer and its hyperparameters.\nExample:\nCopied >>> from transformers import TrainingArguments\n>>> args = TrainingArguments("working_dir")\n>>> args = args.set_optimizer(name="adamw_torch", beta1=0.8)\n>>> args.optim\n\'adamw_torch\' set_push_to_hub\n< source > ( model_id: strstrategy: typing.Union[str, transformers.trainer_utils.HubStrategy] = \'every_save\'token: typing.Optional[str] = Noneprivate_repo: bool = Falsealways_push: bool = False )\nExpand 5 parameters Parameters\nmodel_id (str) —\nThe name of the repository to keep in sync with the local output_dir. It can be a simple model ID in\nwhich case the model will be pushed in your namespace. Otherwise it should be the whole repository\nname, for instance "user_name/model", which allows you to push to an organization you are a member of\nwith "organization_name/model".\nstrategy (str or HubStrategy, optional, defaults to "every_save") —\nDefines the scope of what is pushed to the Hub and when. Possible values are:\n"end": push the model, its configuration, the tokenizer (if passed along to the Trainer) and a\ndraft of a model card when the save_model() method is called.\n"every_save": push the model, its configuration, the tokenizer (if passed along to the Trainer)\nand\na draft of a model card each time there is a model save. The pushes are asynchronous to not block\ntraining, and in case the save are very frequent, a new push is only attempted if the previous one is\nfinished. A last push is made with the final model at the end of training.\n"checkpoint": like "every_save" but the latest checkpoint is also pushed in a subfolder named\nlast-checkpoint, allowing you to resume training easily with\ntrainer.train(resume_from_checkpoint="last-checkpoint").\n"all_checkpoints": like "checkpoint" but all checkpoints are pushed like they appear in the\noutput\nfolder (so you will get one checkpoint folder per folder in your final repository)\ntoken (str, optional) —\nThe token to use to push the model to the Hub. Will default to the token in the cache folder obtained\nwith huggingface-cli login.\nprivate_repo (bool, optional, defaults to False) —\nIf True, the Hub repo will be set to private.\nalways_push (bool, optional, defaults to False) —\nUnless this is True, the Trainer will skip pushing a checkpoint when the previous push is not\nfinished.\nA method that regroups all arguments linked to synchronizing checkpoints with the Hub. Calling this method will set self.push_to_hub to True, which means the output_dir will begin a git\ndirectory synced with the repo (determined by model_id) and the content will be pushed each time a save is\ntriggered (depending onself.save_strategy). Calling save_model() will also trigger a push.\nExample:\nCopied >>> from transformers import TrainingArguments\n>>> args = TrainingArguments("working_dir")\n>>> args = args.set_push_to_hub("me/awesome-model")\n>>> args.hub_model_id\n\'me/awesome-model\' set_save\n< source > ( strategy: typing.Union[str, transformers.trainer_utils.IntervalStrategy] = \'steps\'steps: int = 500total_limit: typing.Optional[int] = Noneon_each_node: bool = False )\nParameters\nstrategy (str or IntervalStrategy, optional, defaults to "steps") —\nThe checkpoint save strategy to adopt during training. Possible values are:\n"no": No save is done during training.\n"epoch": Save is done at the end of each epoch.\n"steps": Save is done every save_steps.\nsteps (int, optional, defaults to 500) —\nNumber of updates steps before two checkpoint saves if strategy="steps".\ntotal_limit (int, optional) —\nIf a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\noutput_dir.\non_each_node (bool, optional, defaults to False) —\nWhen doing multi-node distributed training, whether to save models and checkpoints on each node, or\nonly on the main one.\nThis should not be activated when the different nodes use the same storage as the files will be saved\nwith the same names for each node.\nA method that regroups all arguments linked to checkpoint saving.\nExample:\nCopied >>> from transformers import TrainingArguments\n>>> args = TrainingArguments("working_dir")\n>>> args = args.set_save(strategy="steps", steps=100)\n>>> args.save_steps\n100 set_testing\n< source > ( batch_size: int = 8loss_only: bool = Falsejit_mode: bool = False )\nParameters\nbatch_size (int optional, defaults to 8) —\nThe batch size per device (GPU/TPU core/CPU…) used for testing.\nloss_only (bool, optional, defaults to False) —\nIgnores all outputs except the loss.\njit_mode (bool, optional) —\nWhether or not to use PyTorch jit trace for inference.\nA method that regroups all basic arguments linked to testing on a held-out dataset. Calling this method will automatically set self.do_predict to True.\nExample:\nCopied >>> from transformers import TrainingArguments\n>>> args = TrainingArguments("working_dir")\n>>> args = args.set_testing(batch_size=32)\n>>> args.per_device_eval_batch_size\n32 set_training\n< source > ( learning_rate: float = 5e-05batch_size: int = 8weight_decay: float = 0num_epochs: float = 3max_steps: int = -1gradient_accumulation_steps: int = 1seed: int = 42gradient_checkpointing: bool = False )\nExpand 8 parameters Parameters\nlearning_rate (float, optional, defaults to 5e-5) —\nThe initial learning rate for the optimizer.\nbatch_size (int optional, defaults to 8) —\nThe batch size per device (GPU/TPU core/CPU…) used for training.\nweight_decay (float, optional, defaults to 0) —\nThe weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in the\noptimizer.\nnum_train_epochs(float, optional, defaults to 3.0) —\nTotal number of training epochs to perform (if not an integer, will perform the decimal part percents\nof the last epoch before stopping training).\nmax_steps (int, optional, defaults to -1) —\nIf set to a positive number, the total number of training steps to perform. Overrides\nnum_train_epochs. In case of using a finite iterable dataset the training may stop before reaching\nthe set number of steps when all data is exhausted.\ngradient_accumulation_steps (int, optional, defaults to 1) —\nNumber of updates steps to accumulate the gradients for, before performing a backward/update pass.\nWhen using gradient accumulation, one step is counted as one step with backward pass. Therefore,\nlogging, evaluation, save will be conducted every gradient_accumulation_steps * xxx_step training\nexamples.\nseed (int, optional, defaults to 42) —\nRandom seed that will be set at the beginning of training. To ensure reproducibility across runs, use\nthe ~Trainer.model_init function to instantiate the model if it has some randomly initialized\nparameters.\ngradient_checkpointing (bool, optional, defaults to False) —\nIf True, use gradient checkpointing to save memory at the expense of slower backward pass.\nA method that regroups all basic arguments linked to the training. Calling this method will automatically set self.do_train to True.\nExample:\nCopied >>> from transformers import TrainingArguments\n>>> args = TrainingArguments("working_dir")\n>>> args = args.set_training(learning_rate=1e-4, batch_size=32)\n>>> args.learning_rate\n1e-4 to_dict\n< source > ( )\nSerializes this instance while replace Enum by their values (for JSON serialization support). It obfuscates\nthe token values by removing their value. to_json_string\n< source > ( )\nSerializes this instance to a JSON string. to_sanitized_dict\n< source > ( )\nSanitized serialization to use with TensorBoard’s hparams\nSeq2SeqTrainingArguments class transformers.Seq2SeqTrainingArguments\n< source > ( output_dir: stroverwrite_output_dir: bool = Falsedo_train: bool = Falsedo_eval: bool = Falsedo_predict: bool = Falseevaluation_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = \'no\'prediction_loss_only: bool = Falseper_device_train_batch_size: int = 8per_device_eval_batch_size: int = 8per_gpu_train_batch_size: typing.Optional[int] = Noneper_gpu_eval_batch_size: typing.Optional[int] = Nonegradient_accumulation_steps: int = 1eval_accumulation_steps: typing.Optional[int] = Noneeval_delay: typing.Optional[float] = 0learning_rate: float = 5e-05weight_decay: float = 0.0adam_beta1: float = 0.9adam_beta2: float = 0.999adam_epsilon: float = 1e-08max_grad_norm: float = 1.0num_train_epochs: float = 3.0max_steps: int = -1lr_scheduler_type: typing.Union[transformers.trainer_utils.SchedulerType, str] = \'linear\'warmup_ratio: float = 0.0warmup_steps: int = 0log_level: typing.Optional[str] = \'passive\'log_level_replica: typing.Optional[str] = \'warning\'log_on_each_node: bool = Truelogging_dir: typing.Optional[str] = Nonelogging_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = \'steps\'logging_first_step: bool = Falselogging_steps: float = 500logging_nan_inf_filter: bool = Truesave_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = \'steps\'save_steps: float = 500save_total_limit: typing.Optional[int] = Nonesave_safetensors: typing.Optional[bool] = Truesave_on_each_node: bool = Falseno_cuda: bool = Falseuse_cpu: bool = Falseuse_mps_device: bool = Falseseed: int = 42data_seed: typing.Optional[int] = Nonejit_mode_eval: bool = Falseuse_ipex: bool = Falsebf16: bool = Falsefp16: bool = Falsefp16_opt_level: str = \'O1\'half_precision_backend: str = \'auto\'bf16_full_eval: bool = Falsefp16_full_eval: bool = Falsetf32: typing.Optional[bool] = Nonelocal_rank: int = -1ddp_backend: typing.Optional[str] = Nonetpu_num_cores: typing.Optional[int] = Nonetpu_metrics_debug: bool = Falsedebug: typing.Union[str, typing.List[transformers.debug_utils.DebugOption]] = \'\'dataloader_drop_last: bool = Falseeval_steps: typing.Optional[float] = Nonedataloader_num_workers: int = 0past_index: int = -1run_name: typing.Optional[str] = Nonedisable_tqdm: typing.Optional[bool] = Noneremove_unused_columns: typing.Optional[bool] = Truelabel_names: typing.Optional[typing.List[str]] = Noneload_best_model_at_end: typing.Optional[bool] = Falsemetric_for_best_model: typing.Optional[str] = Nonegreater_is_better: typing.Optional[bool] = Noneignore_data_skip: bool = Falsefsdp: typing.Union[typing.List[transformers.trainer_utils.FSDPOption], str, NoneType] = \'\'fsdp_min_num_params: int = 0fsdp_config: typing.Optional[str] = Nonefsdp_transformer_layer_cls_to_wrap: typing.Optional[str] = Nonedeepspeed: typing.Optional[str] = Nonelabel_smoothing_factor: float = 0.0optim: typing.Union[transformers.training_args.OptimizerNames, str] = \'adamw_torch\'optim_args: typing.Optional[str] = Noneadafactor: bool = Falsegroup_by_length: bool = Falselength_column_name: typing.Optional[str] = \'length\'report_to: typing.Optional[typing.List[str]] = Noneddp_find_unused_parameters: typing.Optional[bool] = Noneddp_bucket_cap_mb: typing.Optional[int] = Noneddp_broadcast_buffers: typing.Optional[bool] = Nonedataloader_pin_memory: bool = Trueskip_memory_metrics: bool = Trueuse_legacy_prediction_loop: bool = Falsepush_to_hub: bool = Falseresume_from_checkpoint: typing.Optional[str] = Nonehub_model_id: typing.Optional[str] = Nonehub_strategy: typing.Union[transformers.trainer_utils.HubStrategy, str] = \'every_save\'hub_token: typing.Optional[str] = Nonehub_private_repo: bool = Falsehub_always_push: bool = Falsegradient_checkpointing: bool = Falsegradient_checkpointing_kwargs: dict = Noneinclude_inputs_for_metrics: bool = Falsefp16_backend: str = \'auto\'push_to_hub_model_id: typing.Optional[str] = Nonepush_to_hub_organization: typing.Optional[str] = Nonepush_to_hub_token: typing.Optional[str] = Nonemp_parameters: str = \'\'auto_find_batch_size: bool = Falsefull_determinism: bool = Falsetorchdynamo: typing.Optional[str] = Noneray_scope: typing.Optional[str] = \'last\'ddp_timeout: typing.Optional[int] = 1800torch_compile: bool = Falsetorch_compile_backend: typing.Optional[str] = Nonetorch_compile_mode: typing.Optional[str] = Nonedispatch_batches: typing.Optional[bool] = Nonesplit_batches: typing.Optional[bool] = Falseinclude_tokens_per_second: typing.Optional[bool] = Falseneftune_noise_alpha: float = Nonesortish_sampler: bool = Falsepredict_with_generate: bool = Falsegeneration_max_length: typing.Optional[int] = Nonegeneration_num_beams: typing.Optional[int] = Nonegeneration_config: typing.Union[str, pathlib.Path, transformers.generation.configuration_utils.GenerationConfig, NoneType] = None )\nExpand 106 parameters Parameters\noutput_dir (str) —\nThe output directory where the model predictions and checkpoints will be written.\noverwrite_output_dir (bool, optional, defaults to False) —\nIf True, overwrite the content of the output directory. Use this to continue training if output_dir\npoints to a checkpoint directory.\ndo_train (bool, optional, defaults to False) —\nWhether to run training or not. This argument is not directly used by Trainer, it’s intended to be used\nby your training/evaluation scripts instead. See the example\nscripts for more details.\ndo_eval (bool, optional) —\nWhether to run evaluation on the validation set or not. Will be set to True if evaluation_strategy is\ndifferent from "no". This argument is not directly used by Trainer, it’s intended to be used by your\ntraining/evaluation scripts instead. See the example\nscripts for more details.\ndo_predict (bool, optional, defaults to False) —\nWhether to run predictions on the test set or not. This argument is not directly used by Trainer, it’s\nintended to be used by your training/evaluation scripts instead. See the example\nscripts for more details.\nevaluation_strategy (str or IntervalStrategy, optional, defaults to "no") —\nThe evaluation strategy to adopt during training. Possible values are:\n"no": No evaluation is done during training.\n"steps": Evaluation is done (and logged) every eval_steps.\n"epoch": Evaluation is done at the end of each epoch.\nprediction_loss_only (bool, optional, defaults to False) —\nWhen performing evaluation and generating predictions, only returns the loss.\nper_device_train_batch_size (int, optional, defaults to 8) —\nThe batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training.\nper_device_eval_batch_size (int, optional, defaults to 8) —\nThe batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.\ngradient_accumulation_steps (int, optional, defaults to 1) —\nNumber of updates steps to accumulate the gradients for, before performing a backward/update pass.\nWhen using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,\nevaluation, save will be conducted every gradient_accumulation_steps * xxx_step training examples.\neval_accumulation_steps (int, optional) —\nNumber of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\nleft unset, the whole predictions are accumulated on GPU/NPU/TPU before being moved to the CPU (faster but\nrequires more memory).\neval_delay (float, optional) —\nNumber of epochs or steps to wait for before the first evaluation can be performed, depending on the\nevaluation_strategy.\nlearning_rate (float, optional, defaults to 5e-5) —\nThe initial learning rate for AdamW optimizer.\nweight_decay (float, optional, defaults to 0) —\nThe weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW\noptimizer.\nadam_beta1 (float, optional, defaults to 0.9) —\nThe beta1 hyperparameter for the AdamW optimizer.\nadam_beta2 (float, optional, defaults to 0.999) —\nThe beta2 hyperparameter for the AdamW optimizer.\nadam_epsilon (float, optional, defaults to 1e-8) —\nThe epsilon hyperparameter for the AdamW optimizer.\nmax_grad_norm (float, optional, defaults to 1.0) —\nMaximum gradient norm (for gradient clipping).\nnum_train_epochs(float, optional, defaults to 3.0) —\nTotal number of training epochs to perform (if not an integer, will perform the decimal part percents of\nthe last epoch before stopping training).\nmax_steps (int, optional, defaults to -1) —\nIf set to a positive number, the total number of training steps to perform. Overrides num_train_epochs.\nIn case of using a finite iterable dataset the training may stop before reaching the set number of steps\nwhen all data is exhausted\nlr_scheduler_type (str or SchedulerType, optional, defaults to "linear") —\nThe scheduler type to use. See the documentation of SchedulerType for all possible values.\nwarmup_ratio (float, optional, defaults to 0.0) —\nRatio of total training steps used for a linear warmup from 0 to learning_rate.\nwarmup_steps (int, optional, defaults to 0) —\nNumber of steps used for a linear warmup from 0 to learning_rate. Overrides any effect of warmup_ratio.\nlog_level (str, optional, defaults to passive) —\nLogger log level to use on the main process. Possible choices are the log levels as strings: ‘debug’,\n‘info’, ‘warning’, ‘error’ and ‘critical’, plus a ‘passive’ level which doesn’t set anything and keeps the\ncurrent log level for the Transformers library (which will be "warning" by default).\nlog_level_replica (str, optional, defaults to "warning") —\nLogger log level to use on replicas. Same choices as log_level”\nlog_on_each_node (bool, optional, defaults to True) —\nIn multinode distributed training, whether to log using log_level once per node, or only on the main\nnode.\nlogging_dir (str, optional) —\nTensorBoard log directory. Will default to\n*output_dir/runs/CURRENT_DATETIME_HOSTNAME*.\nlogging_strategy (str or IntervalStrategy, optional, defaults to "steps") —\nThe logging strategy to adopt during training. Possible values are:\n"no": No logging is done during training.\n"epoch": Logging is done at the end of each epoch.\n"steps": Logging is done every logging_steps.\nlogging_first_step (bool, optional, defaults to False) —\nWhether to log and evaluate the first global_step or not.\nlogging_steps (int or float, optional, defaults to 500) —\nNumber of update steps between two logs if logging_strategy="steps". Should be an integer or a float in\nrange [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\nlogging_nan_inf_filter (bool, optional, defaults to True) —\nWhether to filter nan and inf losses for logging. If set to True the loss of every step that is nan\nor inf is filtered and the average loss of the current logging window is taken instead.\nlogging_nan_inf_filter only influences the logging of loss values, it does not change the behavior the\ngradient is computed or applied to the model.\nsave_strategy (str or IntervalStrategy, optional, defaults to "steps") —\nThe checkpoint save strategy to adopt during training. Possible values are:\n"no": No save is done during training.\n"epoch": Save is done at the end of each epoch.\n"steps": Save is done every save_steps.\nsave_steps (int or float, optional, defaults to 500) —\nNumber of updates steps before two checkpoint saves if save_strategy="steps". Should be an integer or a\nfloat in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.\nsave_total_limit (int, optional) —\nIf a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\noutput_dir. When load_best_model_at_end is enabled, the “best” checkpoint according to\nmetric_for_best_model will always be retained in addition to the most recent ones. For example, for\nsave_total_limit=5 and load_best_model_at_end, the four last checkpoints will always be retained\nalongside the best model. When save_total_limit=1 and load_best_model_at_end, it is possible that two\ncheckpoints are saved: the last one and the best one (if they are different).\nsave_safetensors (bool, optional, defaults to True) —\nUse safetensors saving and loading for state dicts instead of\ndefault torch.load and torch.save.\nsave_on_each_node (bool, optional, defaults to False) —\nWhen doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\nthe main one.\nThis should not be activated when the different nodes use the same storage as the files will be saved with\nthe same names for each node.\nuse_cpu (bool, optional, defaults to False) —\nWhether or not to use cpu. If set to False, we will use cuda or mps device if available.\nseed (int, optional, defaults to 42) —\nRandom seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n~Trainer.model_init function to instantiate the model if it has some randomly initialized parameters.\ndata_seed (int, optional) —\nRandom seed to be used with data samplers. If not set, random generators for data sampling will use the\nsame seed as seed. This can be used to ensure reproducibility of data sampling, independent of the model\nseed.\njit_mode_eval (bool, optional, defaults to False) —\nWhether or not to use PyTorch jit trace for inference.\nuse_ipex (bool, optional, defaults to False) —\nUse Intel extension for PyTorch when it is available. IPEX\ninstallation.\nbf16 (bool, optional, defaults to False) —\nWhether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\nNVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\nfp16 (bool, optional, defaults to False) —\nWhether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\nfp16_opt_level (str, optional, defaults to ‘O1’) —\nFor fp16 training, Apex AMP optimization level selected in [‘O0’, ‘O1’, ‘O2’, and ‘O3’]. See details on\nthe Apex documentation.\nfp16_backend (str, optional, defaults to "auto") —\nThis argument is deprecated. Use half_precision_backend instead.\nhalf_precision_backend (str, optional, defaults to "auto") —\nThe backend to use for mixed precision training. Must be one of "auto", "apex", "cpu_amp". "auto" will\nuse CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the\nrequested backend.\nbf16_full_eval (bool, optional, defaults to False) —\nWhether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\nmetric values. This is an experimental API and it may change.\nfp16_full_eval (bool, optional, defaults to False) —\nWhether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\nmetric values.\ntf32 (bool, optional) —\nWhether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\non PyTorch’s version default of torch.backends.cuda.matmul.allow_tf32. For more details please refer to\nthe TF32 documentation. This is an\nexperimental API and it may change.\nlocal_rank (int, optional, defaults to -1) —\nRank of the process during distributed training.\nddp_backend (str, optional) —\nThe backend to use for distributed training. Must be one of "nccl", "mpi", "ccl", "gloo", "hccl".\ntpu_num_cores (int, optional) —\nWhen training on TPU, the number of TPU cores (automatically passed by launcher script).\ndataloader_drop_last (bool, optional, defaults to False) —\nWhether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\nor not.\neval_steps (int or float, optional) —\nNumber of update steps between two evaluations if evaluation_strategy="steps". Will default to the same\nvalue as logging_steps if not set. Should be an integer or a float in range [0,1). If smaller than 1,\nwill be interpreted as ratio of total training steps.\ndataloader_num_workers (int, optional, defaults to 0) —\nNumber of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\nmain process.\npast_index (int, optional, defaults to -1) —\nSome models like TransformerXL or XLNet can make use of\nthe past hidden states for their predictions. If this argument is set to a positive int, the Trainer will\nuse the corresponding output (usually index 2) as the past state and feed it to the model at the next\ntraining step under the keyword argument mems.\nrun_name (str, optional) —\nA descriptor for the run. Typically used for wandb and\nmlflow logging.\ndisable_tqdm (bool, optional) —\nWhether or not to disable the tqdm progress bars and table of metrics produced by\n~notebook.NotebookTrainingTracker in Jupyter Notebooks. Will default to True if the logging level is\nset to warn or lower (default), False otherwise.\nremove_unused_columns (bool, optional, defaults to True) —\nWhether or not to automatically remove the columns unused by the model forward method.\n(Note that this behavior is not implemented for TFTrainer yet.)\nlabel_names (List[str], optional) —\nThe list of keys in your dictionary of inputs that correspond to the labels.\nWill eventually default to the list of argument names accepted by the model that contain the word “label”,\nexcept if the model used is one of the XxxForQuestionAnswering in which case it will also include the\n["start_positions", "end_positions"] keys.\nload_best_model_at_end (bool, optional, defaults to False) —\nWhether or not to load the best model found during training at the end of training. When this option is\nenabled, the best checkpoint will always be saved. See\nsave_total_limit\nfor more.\nWhen set to True, the parameters save_strategy needs to be the same as evaluation_strategy, and in\nthe case it is “steps”, save_steps must be a round multiple of eval_steps.\nmetric_for_best_model (str, optional) —\nUse in conjunction with load_best_model_at_end to specify the metric to use to compare two different\nmodels. Must be the name of a metric returned by the evaluation with or without the prefix "eval_". Will\ndefault to "loss" if unspecified and load_best_model_at_end=True (to use the evaluation loss).\nIf you set this value, greater_is_better will default to True. Don’t forget to set it to False if\nyour metric is better when lower.\ngreater_is_better (bool, optional) —\nUse in conjunction with load_best_model_at_end and metric_for_best_model to specify if better models\nshould have a greater metric or not. Will default to:\nTrue if metric_for_best_model is set to a value that isn’t "loss" or "eval_loss".\nFalse if metric_for_best_model is not set, or set to "loss" or "eval_loss".\nignore_data_skip (bool, optional, defaults to False) —\nWhen resuming training, whether or not to skip the epochs and batches to get the data loading at the same\nstage as in the previous training. If set to True, the training will begin faster (as that skipping step\ncan take a long time) but will not yield the same results as the interrupted training would have.\nfsdp (bool, str or list of FSDPOption, optional, defaults to \'\') —\nUse PyTorch Distributed Parallel Training (in distributed training only).\nA list of options along the following:\n"full_shard": Shard parameters, gradients and optimizer states.\n"shard_grad_op": Shard optimizer states and gradients.\n"offload": Offload parameters and gradients to CPUs (only compatible with "full_shard" and\n"shard_grad_op").\n"auto_wrap": Automatically recursively wrap layers with FSDP using default_auto_wrap_policy.\nfsdp_config (str or dict, optional) —\nConfig to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location of\ndeepspeed json config file (e.g., ds_config.json) or an already loaded json file as dict.\nA List of config and its options:\nmin_num_params (int, optional, defaults to 0):\nFSDP’s minimum number of parameters for Default Auto Wrapping. (useful only when fsdp field is\npassed).\ntransformer_layer_cls_to_wrap (List[str], optional):\nList of transformer layer class names (case-sensitive) to wrap, e.g, BertLayer, GPTJBlock,\nT5Block … (useful only when fsdp flag is passed).\nbackward_prefetch (str, optional)\nFSDP’s backward prefetch mode. Controls when to prefetch next set of parameters (useful only when\nfsdp field is passed).\nA list of options along the following:\n"backward_pre" : Prefetches the next set of parameters before the current set of parameter’s\ngradient\ncomputation.\n"backward_post" : This prefetches the next set of parameters after the current set of\nparameter’s\ngradient computation.\nforward_prefetch (bool, optional, defaults to False)\nFSDP’s forward prefetch mode (useful only when fsdp field is passed).\nIf "True", then FSDP explicitly prefetches the next upcoming all-gather while executing in the\nforward pass.\nlimit_all_gathers (bool, optional, defaults to False)\nFSDP’s limit_all_gathers (useful only when fsdp field is passed).\nIf "True", FSDP explicitly synchronizes the CPU thread to prevent too many in-flight\nall-gathers.\nuse_orig_params (bool, optional, defaults to False)\nIf "True", allows non-uniform requires_grad during init, which means support for interspersed\nfrozen and trainable paramteres. Useful in cases such as parameter-efficient fine-tuning. Please\nrefer this\n[blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019\nsync_module_states (bool, optional, defaults to True)\nIf "True", each individually wrapped FSDP unit will broadcast module parameters from rank 0 to\nensure they are the same across all ranks after initialization\nxla (bool, optional, defaults to False):\nWhether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental feature\nand its API may evolve in the future.\nxla_fsdp_settings (dict, optional)\nThe value is a dictionary which stores the XLA FSDP wrapping parameters.\nFor a complete list of options, please see here.\nxla_fsdp_grad_ckpt (bool, optional, defaults to False):\nWill use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be\nused when the xla flag is set to true, and an auto wrapping policy is specified through\nfsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.\nactivation_checkpointing (bool, optional, defaults to False):\nIf True, activation checkpointing is a technique to reduce memory usage by clearing activations of\ncertain layers and recomputing them during a backward pass. Effectively, this trades extra\ncomputation time for reduced memory usage.\ndeepspeed (str or dict, optional) —\nUse Deepspeed. This is an experimental feature and its API may\nevolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\nds_config.json) or an already loaded json file as a dict”\nlabel_smoothing_factor (float, optional, defaults to 0.0) —\nThe label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\nlabels are changed from 0s and 1s to label_smoothing_factor/num_labels and 1 - label_smoothing_factor + label_smoothing_factor/num_labels respectively.\ndebug (str or list of DebugOption, optional, defaults to "") —\nEnable one or more debug features. This is an experimental feature.\nPossible options are:\n"underflow_overflow": detects overflow in model’s input/outputs and reports the last frames that led to\nthe event\n"tpu_metrics_debug": print debug metrics on TPU\nThe options should be separated by whitespaces.\noptim (str or training_args.OptimizerNames, optional, defaults to "adamw_torch") —\nThe optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or\nadafactor.\noptim_args (str, optional) —\nOptional arguments that are supplied to AnyPrecisionAdamW.\ngroup_by_length (bool, optional, defaults to False) —\nWhether or not to group together samples of roughly the same length in the training dataset (to minimize\npadding applied and be more efficient). Only useful if applying dynamic padding.\nlength_column_name (str, optional, defaults to "length") —\nColumn name for precomputed lengths. If the column exists, grouping by length will use these values rather\nthan computing them on train startup. Ignored unless group_by_length is True and the dataset is an\ninstance of Dataset.\nreport_to (str or List[str], optional, defaults to "all") —\nThe list of integrations to report the results and logs to. Supported platforms are "azure_ml",\n"clearml", "codecarbon", "comet_ml", "dagshub", "flyte", "mlflow", "neptune",\n"tensorboard", and "wandb". Use "all" to report to all integrations installed, "none" for no\nintegrations.\nddp_find_unused_parameters (bool, optional) —\nWhen using distributed training, the value of the flag find_unused_parameters passed to\nDistributedDataParallel. Will default to False if gradient checkpointing is used, True otherwise.\nddp_bucket_cap_mb (int, optional) —\nWhen using distributed training, the value of the flag bucket_cap_mb passed to DistributedDataParallel.\nddp_broadcast_buffers (bool, optional) —\nWhen using distributed training, the value of the flag broadcast_buffers passed to\nDistributedDataParallel. Will default to False if gradient checkpointing is used, True otherwise.\ndataloader_pin_memory (bool, optional, defaults to True) —\nWhether you want to pin memory in data loaders or not. Will default to True.\nskip_memory_metrics (bool, optional, defaults to True) —\nWhether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\ndown the training and evaluation speed.\npush_to_hub (bool, optional, defaults to False) —\nWhether or not to push the model to the Hub every time the model is saved. If this is activated,\noutput_dir will begin a git directory synced with the repo (determined by hub_model_id) and the content\nwill be pushed each time a save is triggered (depending on your save_strategy). Calling\nsave_model() will also trigger a push.\nIf output_dir exists, it needs to be a local clone of the repository to which the Trainer will be\npushed.\nresume_from_checkpoint (str, optional) —\nThe path to a folder with a valid checkpoint for your model. This argument is not directly used by\nTrainer, it’s intended to be used by your training/evaluation scripts instead. See the example\nscripts for more details.\nhub_model_id (str, optional) —\nThe name of the repository to keep in sync with the local output_dir. It can be a simple model ID in\nwhich case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\nfor instance "user_name/model", which allows you to push to an organization you are a member of with\n"organization_name/model". Will default to user_name/output_dir_name with output_dir_name being the\nname of output_dir.\nWill default to the name of output_dir.\nhub_strategy (str or HubStrategy, optional, defaults to "every_save") —\nDefines the scope of what is pushed to the Hub and when. Possible values are:\n"end": push the model, its configuration, the tokenizer (if passed along to the Trainer) and a\ndraft of a model card when the save_model() method is called.\n"every_save": push the model, its configuration, the tokenizer (if passed along to the Trainer) and\na draft of a model card each time there is a model save. The pushes are asynchronous to not block\ntraining, and in case the save are very frequent, a new push is only attempted if the previous one is\nfinished. A last push is made with the final model at the end of training.\n"checkpoint": like "every_save" but the latest checkpoint is also pushed in a subfolder named\nlast-checkpoint, allowing you to resume training easily with\ntrainer.train(resume_from_checkpoint="last-checkpoint").\n"all_checkpoints": like "checkpoint" but all checkpoints are pushed like they appear in the output\nfolder (so you will get one checkpoint folder per folder in your final repository)\nhub_token (str, optional) —\nThe token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\nhuggingface-cli login.\nhub_private_repo (bool, optional, defaults to False) —\nIf True, the Hub repo will be set to private.\nhub_always_push (bool, optional, defaults to False) —\nUnless this is True, the Trainer will skip pushing a checkpoint when the previous push is not finished.\ngradient_checkpointing (bool, optional, defaults to False) —\nIf True, use gradient checkpointing to save memory at the expense of slower backward pass.\ngradient_checkpointing_args (dict, optional, defaults to None) —\nKey word arguments to be passed to the gradient_checkpointing_enable method.\ninclude_inputs_for_metrics (bool, optional, defaults to False) —\nWhether or not the inputs will be passed to the compute_metrics function. This is intended for metrics\nthat need inputs, predictions and references for scoring calculation in Metric class.\nauto_find_batch_size (bool, optional, defaults to False) —\nWhether to find a batch size that will fit into memory automatically through exponential decay, avoiding\nCUDA Out-of-Memory errors. Requires accelerate to be installed (pip install accelerate)\nfull_determinism (bool, optional, defaults to False) —\nIf True, enable_full_determinism() is called instead of set_seed() to ensure reproducible results in\ndistributed training. Important: this will negatively impact the performance, so only use it for debugging.\ntorchdynamo (str, optional) —\nIf set, the backend compiler for TorchDynamo. Possible choices are "eager", "aot_eager", "inductor",\n"nvfuser", "aot_nvfuser", "aot_cudagraphs", "ofi", "fx2trt", "onnxrt" and "ipex".\nray_scope (str, optional, defaults to "last") —\nThe scope to use when doing hyperparameter search with Ray. By default, "last" will be used. Ray will\nthen use the last checkpoint of all trials, compare those, and select the best one. However, other options\nare also available. See the Ray documentation for\nmore options.\nddp_timeout (int, optional, defaults to 1800) —\nThe timeout for torch.distributed.init_process_group calls, used to avoid GPU socket timeouts when\nperforming slow operations in distributed runnings. Please refer the [PyTorch documentation]\n(https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more\ninformation.\nuse_mps_device (bool, optional, defaults to False) —\nThis argument is deprecated.mps device will be used if it is available similar to cuda device.\ntorch_compile (bool, optional, defaults to False) —\nWhether or not to compile the model using PyTorch 2.0\ntorch.compile.\nThis will use the best defaults for the torch.compile\nAPI.\nYou can customize the defaults with the argument torch_compile_backend and torch_compile_mode but we\ndon’t guarantee any of them will work as the support is progressively rolled in in PyTorch.\nThis flag and the whole compile API is experimental and subject to change in future releases.\ntorch_compile_backend (str, optional) —\nThe backend to use in torch.compile. If set to any value, torch_compile will be set to True.\nRefer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\nThis flag is experimental and subject to change in future releases.\ntorch_compile_mode (str, optional) —\nThe mode to use in torch.compile. If set to any value, torch_compile will be set to True.\nRefer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\nThis flag is experimental and subject to change in future releases.\nsplit_batches (bool, optional) —\nWhether or not the accelerator should split the batches yielded by the dataloaders across the devices\nduring distributed training. If\nset to True, the actual batch size used will be the same on any kind of distributed processes, but it\nmust be a\nround multiple of the number of processes you are using (such as GPUs).\ninclude_tokens_per_second (bool, optional) —\nWhether or not to compute the number of tokens per second per device for training speed metrics.\nThis will iterate over the entire training dataloader once beforehand,\nand will slow down the entire process.\nneftune_noise_alpha (Optional[float]) —\nIf not None, this will activate NEFTune noise embeddings. This can drastically improve model performance\nfor instruction fine-tuning. Check out the original paper and the\noriginal code. Support transformers PreTrainedModel and also\nPeftModel from peft.\nsortish_sampler (bool, optional, defaults to False) —\nWhether to use a sortish sampler or not. Only possible if the underlying datasets are Seq2SeqDataset\nfor now but will become generally available in the near future.\nIt sorts the inputs according to lengths in order to minimize the padding size, with a bit of randomness\nfor the training set.\npredict_with_generate (bool, optional, defaults to False) —\nWhether to use generate to calculate generative metrics (ROUGE, BLEU).\ngeneration_max_length (int, optional) —\nThe max_length to use on each evaluation loop when predict_with_generate=True. Will default to the\nmax_length value of the model configuration.\ngeneration_num_beams (int, optional) —\nThe num_beams to use on each evaluation loop when predict_with_generate=True. Will default to the\nnum_beams value of the model configuration.\ngeneration_config (str or Path or GenerationConfig, optional) —\nAllows to load a GenerationConfig from the from_pretrained method. This can be either:\na string, the model id of a pretrained model configuration hosted inside a model repo on\nhuggingface.co. Valid model ids can be located at the root-level, like bert-base-uncased, or namespaced\nunder a user or organization name, like dbmdz/bert-base-german-cased.\na path to a directory containing a configuration file saved using the\nsave_pretrained() method, e.g., ./my_model_directory/.\na GenerationConfig object.\nTrainingArguments is the subset of the arguments we use in our example scripts which relate to the training loop\nitself. Using HfArgumentParser we can turn this class into\nargparse arguments that can be specified on the\ncommand line. to_dict\n< source > ( )\nSerializes this instance while replace Enum by their values and GenerationConfig by dictionaries (for JSON\nserialization support). It obfuscates the token values by removing their value.\nCheckpoints By default, Trainer will save all checkpoints in the output_dir you set in the\nTrainingArguments you are using. Those will go in subfolder named checkpoint-xxx with xxx\nbeing the step at which the training was at. Resuming training from a checkpoint can be done when calling Trainer.train() with either: resume_from_checkpoint=True which will resume training from the latest checkpoint resume_from_checkpoint=checkpoint_dir which will resume training from the specific checkpoint in the directory\npassed. In addition, you can easily save your checkpoints on the Model Hub when using push_to_hub=True. By default, all\nthe models saved in intermediate checkpoints are saved in different commits, but not the optimizer state. You can adapt\nthe hub-strategy value of your TrainingArguments to either: "checkpoint": the latest checkpoint is also pushed in a subfolder named last-checkpoint, allowing you to\nresume training easily with trainer.train(resume_from_checkpoint="output_dir/last-checkpoint"). "all_checkpoints": all checkpoints are pushed like they appear in the output folder (so you will get one\ncheckpoint folder per folder in your final repository)\nLogging By default Trainer will use logging.INFO for the main process and logging.WARNING for the replicas if any. These defaults can be overridden to use any of the 5 logging levels with TrainingArguments’s\narguments: log_level - for the main process log_level_replica - for the replicas Further, if TrainingArguments’s log_on_each_node is set to False only the main node will\nuse the log level settings for its main process, all other nodes will use the log level settings for replicas. Note that Trainer is going to set transformers’s log level separately for each node in its\nTrainer.__init__(). So you may want to set this sooner (see the next example) if you tap into other\ntransformers functionality before creating the Trainer object. Here is an example of how this can be used in an application:\nCopied [...]\nlogger = logging.getLogger(__name__)\n# Setup logging\nlogging.basicConfig(\nformat="%(asctime)s - %(levelname)s - %(name)s - %(message)s",\ndatefmt="%m/%d/%Y %H:%M:%S",\nhandlers=[logging.StreamHandler(sys.stdout)],\n)\n# set the main code and the modules it uses to the same log-level according to the node\nlog_level = training_args.get_process_log_level()\nlogger.setLevel(log_level)\ndatasets.utils.logging.set_verbosity(log_level)\ntransformers.utils.logging.set_verbosity(log_level)\ntrainer = Trainer(...) And then if you only want to see warnings on the main node and all other nodes to not print any most likely duplicated\nwarnings you could run it as:\nCopied my_app.py ... --log_level warning --log_level_replica error In the multi-node environment if you also don’t want the logs to repeat for each node’s main process, you will want to\nchange the above to:\nCopied my_app.py ... --log_level warning --log_level_replica error --log_on_each_node 0 and then only the main process of the first node will log at the “warning” level, and all other processes on the main\nnode and all processes on other nodes will log at the “error” level. If you need your application to be as quiet as possible you could do:\nCopied my_app.py ... --log_level error --log_level_replica error --log_on_each_node 0 (add --log_on_each_node 0 if on multi-node environment)\nRandomness When resuming from a checkpoint generated by Trainer all efforts are made to restore the\npython, numpy and pytorch RNG states to the same states as they were at the moment of saving that checkpoint,\nwhich should make the “stop and resume” style of training as close as possible to non-stop training. However, due to various default non-deterministic pytorch settings this might not fully work. If you want full\ndeterminism please refer to Controlling sources of randomness. As explained in the document, that some of those settings\nthat make things deterministic (.e.g., torch.backends.cudnn.deterministic) may slow things down, therefore this\ncan’t be done by default, but you can enable those yourself if needed.\nSpecific GPUs Selection Let’s discuss how you can tell your program which GPUs are to be used and in what order. When using DistributedDataParallel to use only a subset of your GPUs, you simply specify the number of GPUs to use. For example, if you have 4 GPUs, but you wish to use the first 2 you can do:\nCopied python -m torch.distributed.launch --nproc_per_node=2\ntrainer-program.py ... if you have either accelerate or deepspeed installed you can also accomplish the same by using one of:\nCopied accelerate launch --num_processes 2 trainer-program.py ...\nCopied deepspeed --num_gpus 2 trainer-program.py ... You don’t need to use the Accelerate or the Deepspeed integration features to use these launchers. Until now you were able to tell the program how many GPUs to use. Now let’s discuss how to select specific GPUs and control their order. The following environment variables help you control which GPUs to use and their order. CUDA_VISIBLE_DEVICES If you have multiple GPUs and you’d like to use only 1 or a few of those GPUs, set the environment variable CUDA_VISIBLE_DEVICES to a list of the GPUs to be used. For example, let’s say you have 4 GPUs: 0, 1, 2 and 3. To run only on the physical GPUs 0 and 2, you can do:\nCopied CUDA_VISIBLE_DEVICES=0,2 python -m torch.distributed.launch trainer-program.py ... So now pytorch will see only 2 GPUs, where your physical GPUs 0 and 2 are mapped to cuda:0 and cuda:1 correspondingly. You can even change their order:\nCopied CUDA_VISIBLE_DEVICES=2,0 python -m torch.distributed.launch trainer-program.py ... Here your physical GPUs 0 and 2 are mapped to cuda:1 and cuda:0 correspondingly. The above examples were all for DistributedDataParallel use pattern, but the same method works for DataParallel as well:\nCopied CUDA_VISIBLE_DEVICES=2,0 python trainer-program.py ... To emulate an environment without GPUs simply set this environment variable to an empty value like so:\nCopied CUDA_VISIBLE_DEVICES= python trainer-program.py ... As with any environment variable you can, of course, export those instead of adding these to the command line, as in:\nCopied export CUDA_VISIBLE_DEVICES=0,2\npython -m torch.distributed.launch trainer-program.py ... but this approach can be confusing since you may forget you set up the environment variable earlier and not understand why the wrong GPUs are used. Therefore, it’s a common practice to set the environment variable just for a specific run on the same command line as it’s shown in most examples of this section. CUDA_DEVICE_ORDER There is an additional environment variable CUDA_DEVICE_ORDER that controls how the physical devices are ordered. The two choices are: ordered by PCIe bus IDs (matches nvidia-smi’s order) - this is the default.\nCopied export CUDA_DEVICE_ORDER=PCI_BUS_ID ordered by GPU compute capabilities\nCopied export CUDA_DEVICE_ORDER=FASTEST_FIRST Most of the time you don’t need to care about this environment variable, but it’s very helpful if you have a lopsided setup where you have an old and a new GPUs physically inserted in such a way so that the slow older card appears to be first. One way to fix that is to swap the cards. But if you can’t swap the cards (e.g., if the cooling of the devices gets impacted) then setting CUDA_DEVICE_ORDER=FASTEST_FIRST will always put the newer faster card first. It’ll be somewhat confusing though since nvidia-smi will still report them in the PCIe order. The other solution to swapping the order is to use:\nCopied export CUDA_VISIBLE_DEVICES=1,0 In this example we are working with just 2 GPUs, but of course the same would apply to as many GPUs as your computer has. Also if you do set this environment variable it’s the best to set it in your ~/.bashrc file or some other startup config file and forget about it.\nTrainer Integrations The Trainer has been extended to support libraries that may dramatically improve your training\ntime and fit much bigger models. Currently it supports third party solutions, DeepSpeed and PyTorch FSDP, which implement parts of the paper ZeRO: Memory Optimizations\nToward Training Trillion Parameter Models, by Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. This provided support is new and experimental as of this writing. While the support for DeepSpeed and PyTorch FSDP is active and we welcome issues around it, we don’t support the FairScale integration anymore since it has been integrated in PyTorch main (see the PyTorch FSDP integration)\nCUDA Extension Installation Notes As of this writing, Deepspeed require compilation of CUDA C++ code, before it can be used. While all installation issues should be dealt with through the corresponding GitHub Issues of Deepspeed, there are a few common issues that one may encounter while building\nany PyTorch extension that needs to build CUDA extensions. Therefore, if you encounter a CUDA-related build issue while doing the following:\nCopied pip install deepspeed please, read the following notes first. In these notes we give examples for what to do when pytorch has been built with CUDA 10.2. If your situation is\ndifferent remember to adjust the version number to the one you are after.\nPossible problem #1 While, Pytorch comes with its own CUDA toolkit, to build these two projects you must have an identical version of CUDA\ninstalled system-wide. For example, if you installed pytorch with cudatoolkit==10.2 in the Python environment, you also need to have\nCUDA 10.2 installed system-wide. The exact location may vary from system to system, but /usr/local/cuda-10.2 is the most common location on many\nUnix systems. When CUDA is correctly set up and added to the PATH environment variable, one can find the\ninstallation location by doing:\nCopied which nvcc If you don’t have CUDA installed system-wide, install it first. You will find the instructions by using your favorite\nsearch engine. For example, if you’re on Ubuntu you may want to search for: ubuntu cuda 10.2 install.\nPossible problem #2 Another possible common problem is that you may have more than one CUDA toolkit installed system-wide. For example you\nmay have:\nCopied /usr/local/cuda-10.2\n/usr/local/cuda-11.0 Now, in this situation you need to make sure that your PATH and LD_LIBRARY_PATH environment variables contain\nthe correct paths to the desired CUDA version. Typically, package installers will set these to contain whatever the\nlast version was installed. If you encounter the problem, where the package build fails because it can’t find the right\nCUDA version despite you having it installed system-wide, it means that you need to adjust the 2 aforementioned\nenvironment variables. First, you may look at their contents:\nCopied echo $PATH\necho $LD_LIBRARY_PATH so you get an idea of what is inside. It’s possible that LD_LIBRARY_PATH is empty. PATH lists the locations of where executables can be found and LD_LIBRARY_PATH is for where shared libraries\nare to looked for. In both cases, earlier entries have priority over the later ones. : is used to separate multiple\nentries. Now, to tell the build program where to find the specific CUDA toolkit, insert the desired paths to be listed first by\ndoing:\nCopied export PATH=/usr/local/cuda-10.2/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH Note that we aren’t overwriting the existing values, but prepending instead. Of course, adjust the version number, the full path if need be. Check that the directories you assign actually do\nexist. lib64 sub-directory is where the various CUDA .so objects, like libcudart.so reside, it’s unlikely\nthat your system will have it named differently, but if it is adjust it to reflect your reality.\nPossible problem #3 Some older CUDA versions may refuse to build with newer compilers. For example, you my have gcc-9 but it wants\ngcc-7. There are various ways to go about it. If you can install the latest CUDA toolkit it typically should support the newer compiler. Alternatively, you could install the lower version of the compiler in addition to the one you already have, or you may\nalready have it but it’s not the default one, so the build system can’t see it. If you have gcc-7 installed but the\nbuild system complains it can’t find it, the following might do the trick:\nCopied sudo ln -s /usr/bin/gcc-7\n/usr/local/cuda-10.2/bin/gcc\nsudo ln -s /usr/bin/g++-7\n/usr/local/cuda-10.2/bin/g++ Here, we are making a symlink to gcc-7 from /usr/local/cuda-10.2/bin/gcc and since\n/usr/local/cuda-10.2/bin/ should be in the PATH environment variable (see the previous problem’s solution), it\nshould find gcc-7 (and g++7) and then the build will succeed. As always make sure to edit the paths in the example to match your situation.\nPyTorch Fully Sharded Data parallel To accelerate training huge models on larger batch sizes, we can use a fully sharded data parallel model.\nThis type of data parallel paradigm enables fitting more data and larger models by sharding the optimizer states, gradients and parameters.\nTo read more about it and the benefits, check out the Fully Sharded Data Parallel blog.\nWe have integrated the latest PyTorch’s Fully Sharded Data Parallel (FSDP) training feature.\nAll you need to do is enable it through the config. Required PyTorch version for FSDP support: PyTorch Nightly (or 1.12.0 if you read this after it has been released)\nas the model saving with FSDP activated is only available with recent fixes. Usage: Make sure you have added the distributed launcher\n-m torch.distributed.launch --nproc_per_node=NUMBER_OF_GPUS_YOU_HAVE if you haven’t been using it already. Sharding Strategy: FULL_SHARD : Shards optimizer states + gradients + model parameters across data parallel workers/GPUs.\nFor this, add --fsdp full_shard to the command line arguments. SHARD_GRAD_OP : Shards optimizer states + gradients across data parallel workers/GPUs.\nFor this, add --fsdp shard_grad_op to the command line arguments. NO_SHARD : No sharding. For this, add --fsdp no_shard to the command line arguments. To offload the parameters and gradients to the CPU,\nadd --fsdp "full_shard offload" or --fsdp "shard_grad_op offload" to the command line arguments. To automatically recursively wrap layers with FSDP using default_auto_wrap_policy,\nadd --fsdp "full_shard auto_wrap" or --fsdp "shard_grad_op auto_wrap" to the command line arguments. To enable both CPU offloading and auto wrapping,\nadd --fsdp "full_shard offload auto_wrap" or --fsdp "shard_grad_op offload auto_wrap" to the command line arguments. Remaining FSDP config is passed via --fsdp_config <path_to_fsdp_config.json>. It is either a location of\nFSDP json config file (e.g., fsdp_config.json) or an already loaded json file as dict. If auto wrapping is enabled, you can either use transformer based auto wrap policy or size based auto wrap policy.For transformer based auto wrap policy, it is recommended to specify fsdp_transformer_layer_cls_to_wrap in the config file. If not specified, the default value is model._no_split_modules when available.\nThis specifies the list of transformer layer class name (case-sensitive) to wrap ,e.g, BertLayer, GPTJBlock, T5Block …\nThis is important because submodules that share weights (e.g., embedding layer) should not end up in different FSDP wrapped units.\nUsing this policy, wrapping happens for each block containing Multi-Head Attention followed by couple of MLP layers.\nRemaining layers including the shared embeddings are conveniently wrapped in same outermost FSDP unit.\nTherefore, use this for transformer based models. For size based auto wrap policy, please add fsdp_min_num_params in the config file.\nIt specifies FSDP’s minimum number of parameters for auto wrapping. fsdp_backward_prefetch can be specified in the config file. It controls when to prefetch next set of parameters.\nbackward_pre and backward_pos are available options.\nFor more information refer torch.distributed.fsdp.fully_sharded_data_parallel.BackwardPrefetch fsdp_forward_prefetch can be specified in the config file. It controls when to prefetch next set of parameters.\nIf "True", FSDP explicitly prefetches the next upcoming all-gather while executing in the forward pass. limit_all_gathers can be specified in the config file.\nIf "True", FSDP explicitly synchronizes the CPU thread to prevent too many in-flight all-gathers. activation_checkpointing can be specified in the config file.\nIf "True", FSDP activation checkpointing is a technique to reduce memory usage by clearing activations of\ncertain layers and recomputing them during a backward pass. Effectively, this trades extra computation time\nfor reduced memory usage. Few caveats to be aware of it is incompatible with generate, thus is incompatible with --predict_with_generate\nin all seq2seq/clm scripts (translation/summarization/clm etc.).\nPlease refer issue #21667\nPyTorch/XLA Fully Sharded Data parallel For all the TPU users, great news! PyTorch/XLA now supports FSDP.\nAll the latest Fully Sharded Data Parallel (FSDP) training are supported.\nFor more information refer to the Scaling PyTorch models on Cloud TPUs with FSDP and PyTorch/XLA implementation of FSDP\nAll you need to do is enable it through the config. Required PyTorch/XLA version for FSDP support: >=2.0 Usage: Pass --fsdp "full shard" along with following changes to be made in --fsdp_config <path_to_fsdp_config.json>: xla should be set to True to enable PyTorch/XLA FSDP. xla_fsdp_settings The value is a dictionary which stores the XLA FSDP wrapping parameters.\nFor a complete list of options, please see here. xla_fsdp_grad_ckpt. When True, uses gradient checkpointing over each nested XLA FSDP wrapped layer.\nThis setting can only be used when the xla flag is set to true, and an auto wrapping policy is specified through\nfsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap. You can either use transformer based auto wrap policy or size based auto wrap policy.For transformer based auto wrap policy, it is recommended to specify fsdp_transformer_layer_cls_to_wrap in the config file. If not specified, the default value is model._no_split_modules when available.\nThis specifies the list of transformer layer class name (case-sensitive) to wrap ,e.g, BertLayer, GPTJBlock, T5Block …\nThis is important because submodules that share weights (e.g., embedding layer) should not end up in different FSDP wrapped units.\nUsing this policy, wrapping happens for each block containing Multi-Head Attention followed by couple of MLP layers.\nRemaining layers including the shared embeddings are conveniently wrapped in same outermost FSDP unit.\nTherefore, use this for transformer based models. For size based auto wrap policy, please add fsdp_min_num_params in the config file.\nIt specifies FSDP’s minimum number of parameters for auto wrapping.\nUsing Trainer for accelerated PyTorch Training on Mac With PyTorch v1.12 release, developers and researchers can take advantage of Apple silicon GPUs for significantly faster model training.\nThis unlocks the ability to perform machine learning workflows like prototyping and fine-tuning locally, right on Mac.\nApple’s Metal Performance Shaders (MPS) as a backend for PyTorch enables this and can be used via the new "mps" device.\nThis will map computational graphs and primitives on the MPS Graph framework and tuned kernels provided by MPS.\nFor more information please refer official documents Introducing Accelerated PyTorch Training on Mac\nand MPS BACKEND. We strongly recommend to install PyTorch >= 1.13 (nightly version at the time of writing) on your MacOS machine.\nIt has major fixes related to model correctness and performance improvements for transformer based models.\nPlease refer to https://github.com/pytorch/pytorch/issues/82707 for more details. Benefits of Training and Inference using Apple Silicon Chips Enables users to train larger networks or batch sizes locally Reduces data retrieval latency and provides the GPU with direct access to the full memory store due to unified memory architecture.\nTherefore, improving end-to-end performance. Reduces costs associated with cloud-based development or the need for additional local GPUs. Pre-requisites: To install torch with mps support,\nplease follow this nice medium article GPU-Acceleration Comes to PyTorch on M1 Macs. Usage:\nmps device will be used by default if available similar to the way cuda device is used.\nTherefore, no action from user is required.\nFor example, you can run the official Glue text classififcation task (from the root folder) using Apple Silicon GPU with below command:\nCopied export TASK_NAME=mrpc\npython examples/pytorch/text-classification/run_glue.py \\\n--model_name_or_path bert-base-cased \\\n--task_name $TASK_NAME \\\n--do_train \\\n--do_eval \\\n--max_seq_length 128 \\\n--per_device_train_batch_size 32 \\\n--learning_rate 2e-5 \\\n--num_train_epochs 3 \\\n--output_dir /tmp/$TASK_NAME/ \\\n--overwrite_output_dir A few caveats to be aware of Some PyTorch operations have not been implemented in mps and will throw an error.\nOne way to get around that is to set the environment variable PYTORCH_ENABLE_MPS_FALLBACK=1,\nwhich will fallback to CPU for these operations. It still throws a UserWarning however. Distributed setups gloo and nccl are not working with mps device.\nThis means that currently only single GPU of mps device type can be used. Finally, please, remember that, 🤗 Trainer only integrates MPS backend, therefore if you\nhave any problems or questions with regards to MPS backend usage, please,\nfile an issue with PyTorch GitHub.\nUsing Accelerate Launcher with Trainer Accelerate now powers Trainer. In terms of what users should expect: They can keep using the Trainer ingterations such as FSDP, DeepSpeed vis trainer arguments without any changes on their part. They can now use Accelerate Launcher with Trainer (recommended). Steps to use Accelerate Launcher with Trainer: Make sure 🤗 Accelerate is installed, you can’t use the Trainer without it anyway. If not pip install accelerate. You may also need to update your version of Accelerate: pip install accelerate --upgrade Run accelerate config and fill the questionnaire. Below are example accelerate configs:\na. DDP Multi-node Multi-GPU config:\nCopied compute_environment: LOCAL_MACHINE\ndistributed_type: MULTI_GPU\ndowncast_bf16: \'no\'\ngpu_ids: all\nmachine_rank: 0 #change rank as per the node\nmain_process_ip: 192.168.20.1\nmain_process_port: 9898\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 2\nnum_processes: 8\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false b. FSDP config:\nCopied compute_environment: LOCAL_MACHINE\ndistributed_type: FSDP\ndowncast_bf16: \'no\'\nfsdp_config:\nfsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\nfsdp_backward_prefetch_policy: BACKWARD_PRE\nfsdp_forward_prefetch: true\nfsdp_offload_params: false\nfsdp_sharding_strategy: 1\nfsdp_state_dict_type: FULL_STATE_DICT\nfsdp_sync_module_states: true\nfsdp_transformer_layer_cls_to_wrap: BertLayer\nfsdp_use_orig_params: true\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false c. DeepSpeed config pointing to a file:\nCopied compute_environment: LOCAL_MACHINE\ndeepspeed_config:\ndeepspeed_config_file: /home/user/configs/ds_zero3_config.json\nzero3_init_flag: true\ndistributed_type: DEEPSPEED\ndowncast_bf16: \'no\'\nmachine_rank: 0\nmain_training_function: main\nnum_machines: 1\nnum_processes: 4\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false d. DeepSpeed config using accelerate plugin:\nCopied compute_environment: LOCAL_MACHINE\ndeepspeed_config:\ngradient_accumulation_steps: 1\ngradient_clipping: 0.7\noffload_optimizer_device: cpu\noffload_param_device: cpu\nzero3_init_flag: true\nzero_stage: 2\ndistributed_type: DEEPSPEED\ndowncast_bf16: \'no\'\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 4\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false Run the Trainer script with args other than the ones handled above by accelerate config or launcher args.\nBelow is an example to run run_glue.py using accelerate launcher with FSDP config from above.\nCopied cd transformers\naccelerate launch \\\n./examples/pytorch/text-classification/run_glue.py \\\n--model_name_or_path bert-base-cased \\\n--task_name $TASK_NAME \\\n--do_train \\\n--do_eval \\\n--max_seq_length 128 \\\n--per_device_train_batch_size 16 \\\n--learning_rate 5e-5 \\\n--num_train_epochs 3 \\\n--output_dir /tmp/$TASK_NAME/ \\\n--overwrite_output_dir You can also directly use the cmd args for accelerate launch. Above example would map to:\nCopied cd transformers\naccelerate launch --num_processes=2 \\\n--use_fsdp \\\n--mixed_precision=bf16 \\\n--fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP\n\\\n--fsdp_transformer_layer_cls_to_wrap="BertLayer" \\\n--fsdp_sharding_strategy=1 \\\n--fsdp_state_dict_type=FULL_STATE_DICT \\\n./examples/pytorch/text-classification/run_glue.py\n--model_name_or_path bert-base-cased \\\n--task_name $TASK_NAME \\\n--do_train \\\n--do_eval \\\n--max_seq_length 128 \\\n--per_device_train_batch_size 16 \\\n--learning_rate 5e-5 \\\n--num_train_epochs 3 \\\n--output_dir /tmp/$TASK_NAME/ \\\n--overwrite_output_dir For more information, please refer the 🤗 Accelerate CLI guide: Launching your 🤗 Accelerate scripts. Sections that were moved: [ DeepSpeed\n| Installation\n| Deployment with multiple GPUs\n| Deployment with one GPU\n| Deployment in Notebooks\n| Configuration\n| Passing Configuration\n| Shared Configuration\n| ZeRO\n| ZeRO-2 Config\n| ZeRO-3 Config\n| NVMe Support\n| ZeRO-2 vs ZeRO-3 Performance\n| ZeRO-2 Example\n| ZeRO-3 Example\n| Optimizer\n| Scheduler\n| fp32 Precision\n| Automatic Mixed Precision\n| Batch Size\n| Gradient Accumulation\n| Gradient Clipping\n| Getting The Model Weights Out\n]\nBoost your fine-tuning performances using NEFTune NEFTune is a technique to boost the performance of chat models and was introduced by the paper “NEFTune: Noisy Embeddings Improve Instruction Finetuning” from Jain et al. it consists of adding noise to the embedding vectors during training. According to the abstract of the paper: Standard finetuning of LLaMA-2-7B using Alpaca achieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets. Models trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8% improvement, and with OpenPlatypus an 8% improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune.\nTo use it in Trainer simply pass neftune_noise_alpha when creating your TrainingArguments instance. Note that to avoid any surprising behaviour, NEFTune is disabled after training to retrieve back the original behaviour of the embedding layer.\nCopied from transformers import Trainer, TrainingArguments\nargs = TrainingArguments(..., neftune_noise_alpha=0.1)\ntrainer = Trainer(..., args=args)\n...\ntrainer.train()\n←Tokenizer\nDeepSpeed Integration→\nTrainer Trainer Seq2SeqTrainer TrainingArguments Seq2SeqTrainingArguments Checkpoints Logging Randomness Specific GPUs Selection Trainer Integrations CUDA Extension Installation Notes Possible problem #1Possible problem #2Possible problem #3PyTorch Fully Sharded Data parallel PyTorch/XLA Fully Sharded Data parallel Using Trainer for accelerated PyTorch Training on Mac Using Accelerate Launcher with Trainer Boost your fine-tuning performances using NEFTune', [' Hugging Face (https://huggingface.co/)', ' Models (https://huggingface.co/models)', ' Datasets (https://huggingface.co/datasets)', ' Spaces (https://huggingface.co/spaces)', ' Docs (https://huggingface.co/docs)', 'Pricing (https://huggingface.co/pricing)', 'Log In (https://huggingface.co/login)', 'Sign Up (https://huggingface.co/join)', ' 114,792 (https://github.com/huggingface/transformers)', '🤗 Transformers  (https://huggingface.co/docs/transformers/index)', 'Quick tour  (https://huggingface.co/docs/transformers/quicktour)', 'Installation  (https://huggingface.co/docs/transformers/installation)', 'Run inference with pipelines  (https://huggingface.co/docs/transformers/pipeline_tutorial)', 'Write portable code with AutoClass  (https://huggingface.co/docs/transformers/autoclass_tutorial)', 'Preprocess data  (https://huggingface.co/docs/transformers/preprocessing)', 'Fine-tune a pretrained model  (https://huggingface.co/docs/transformers/training)', 'Train with a script  (https://huggingface.co/docs/transformers/run_scripts)', 'Set up distributed training with 🤗 Accelerate  (https://huggingface.co/docs/transformers/accelerate)', 'Load and train adapters with 🤗 PEFT  (https://huggingface.co/docs/transformers/peft)', 'Share your model  (https://huggingface.co/docs/transformers/model_sharing)'])


import pdb;pdb.set_trace()